---
layout: post
title: Matrix Algebra and Multiple Regressions. 
description: "Matrix Algebra and Multiple Regressions"
category: teaching
tags: [IE14, teaching, week1]
comments: true  
published: true
status: process
---



Multiple regression Model in matrix form
========================================================


{% highlight text %}
## Error: object 'base.url' not found
{% endhighlight %}



{% highlight text %}
## Error: object 'input' not found
{% endhighlight %}



{% highlight text %}
## Error: object 'fig.path' not found
{% endhighlight %}



OLS estimation with matrix algebra in R
---------------------------------------------

Let's look at a regression model with _k=3_ regressors and _n=100_ observations. We can generate a matrix _X_ of dimensions \\(n\times k\\) filled with random normal values. For that we need the following _R_ functions:

* _matrix()_ to generate a matrix.
* _rnrom()_ to generate \\(\mathcal{N}(0,1)\\) random numbers.
* _dim()_ to print the dimensions of a matrix.


{% highlight r %}
# Choosing the seed
set.seed(666)

# Setting up the number of variables and observations:
k <- 3
n <- 100

# Creating the matrix of regressors:
X <- matrix(rnorm(k * n), ncol = 3, nrow = 100)

# printing the dimensions of X:
dim(X)
{% endhighlight %}



{% highlight text %}
## [1] 100   3
{% endhighlight %}


Now let's assume the response variable follows a regression model:

$$ 
	\begin{aligned}
	y & = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon \\
	y & = 1 + 0.5 x_1 + x_2 - x_3 + \epsilon \\
	y & = X\beta + \epsilon
	\end{aligned}
$$

where \\( \epsilon_i \sim \mathcal{N}(0,1)\\). We can now generate the variable _y_ using the matrix multiplication operator _%*%_


{% highlight r %}
# First I generate the innovations:
eps <- rnorm(n)

# Then y:
y <- 1 + X %*% matrix(c(0.5, 1, -1), ncol = 1, nrow = 3) + eps

# And check that the dimensions make sense
dim(y)
{% endhighlight %}



{% highlight text %}
## [1] 100   1
{% endhighlight %}



The generic function to estimate OLS in *R* is called _lm()_, notice that it automatically adds a constant to our model.


{% highlight r %}
ols.lm <- lm(y ~ X)
summary(ols.lm)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.905 -0.626 -0.110  0.675  2.171 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   0.9873     0.0987   10.00  < 2e-16 ***
## X1            0.4840     0.0972    4.98  2.8e-06 ***
## X2            1.2524     0.0881   14.21  < 2e-16 ***
## X3           -0.9078     0.0986   -9.20  7.6e-15 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.979 on 96 degrees of freedom
## Multiple R-squared:  0.78,	Adjusted R-squared:  0.773 
## F-statistic:  113 on 3 and 96 DF,  p-value: <2e-16
{% endhighlight %}


The summary of the _lm()_ function shows many statistics that we will learn to understand throughout the course. If you try _plot(ols.lm)_ you will also get a number of useful plots, but... This is all too easy! 


It is more fun to code the OLS estimator ourselves. Remember the formula for the OLS estimator \\(\hat{\beta}\\) of \\(\beta\\) in matrix form: \\(\hat{\beta}=\left(X'X\right)^{-1}X'y\\). For that we need only two functions: 

* The transpose function: _t()_
* The matrix inversion function _solve()_

We also need to remember adding a constant to X using the _cbind()_ function. 



{% highlight r %}
# Adding the constant
X <- cbind(1, X)

# Estimating beta hat
beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y
print(beta.hat)
{% endhighlight %}



{% highlight text %}
##         [,1]
## [1,]  0.9873
## [2,]  0.4840
## [3,]  1.2524
## [4,] -0.9078
{% endhighlight %}


The estimates of \\(\hat{\beta}\\) are the same using both methods, that is nice. 

*******************

Question:
---------
Compute the standard errors of the residuals and of the OLS estimator. To do this, you will need to:

1. Compute the OLS residuals:

  * compute the fit \\( \hat{y}=X\hat{\beta} \\),
  * compute the residuals \\( \hat{\epsilon} = y - \hat{y} \\). This should be a vector of the same length as _y_.
2. Compute \\(  \hat{\sigma}^2=\frac{\hat{\epsilon}'\hat{\epsilon}}{n-(k+1)}  \\), to get the standard error you need to use the _sqrt()_ function on \\(\hat{\sigma}^2\\)
3. Get the individual standard errors:

  * Get the diagonal of \\((X'X)^{-1}\\) using _diag()_.
  * Scale its square-root by \\(\hat{\sigma}\\). You should get a vector of standard errors of the same length as _beta.hat_.

You can compare your results with those you get running _lm()_ as above.

 > Do not compare your results with those in this page. Replicate the calculation above on your machine, and compare the output of _lm()_ with that of your code.


{% highlight r %}
# Computing the residuals
y.hat <- X %*% beta.hat
e.hat <- y - y.hat

# Residual standard error
sig2 <- (t(e.hat) %*% e.hat)/(n - (k + 1))
res.se <- sqrt(sig2)
print(res.se)
{% endhighlight %}



{% highlight text %}
##        [,1]
## [1,] 0.9785
{% endhighlight %}



{% highlight r %}

# Parameter std. err.
diag.xx <- diag(solve(t(X) %*% X))
beta.se <- res.se * sqrt(diag.xx)
print(beta.se)
{% endhighlight %}



{% highlight text %}
## [1] 0.09869 0.09717 0.08813 0.09863
{% endhighlight %}



--------------------------------

Let evaluate visualy the quality of our model. To do that we can look at a scatter plot of the observed values and the residuals.


{% highlight r %}
# Creating a data frame with the resituals, and the observed and fitted
# values.
ols.man <- data.frame(residuals = e.hat, fitted = y.hat, observed = y)
# Loading a plotting library.
require(ggplot2)
sqplot <- ggplot(ols.man, aes(y = observed, x = residuals)) + geom_point()
{% endhighlight %}



{% highlight r %}
print(sqplot)
{% endhighlight %}

![center](figure/fig-sq.png) 






