---
layout: post
title: 'Week 1: Multiple Regressions and Matrix Notation.' 
description: "Week 1: Multiple Regressions and Matrix Notation."
category: ie
tags: [IE14, teaching, week1]
comments: true  
published: true
status: process
---



Regressions with multiple variables and matrix notation
========================================================

The plan for this week is:

 * Regressions with multiple variables.
 * Get familiar working with matrices.
 * Derive the OLS estimator and its properties using matrix notation.


### Generating data following a regression model with multiple variables
Let us assume the response variable _y_ follows a regression model with 3 explanatory variables:

$$ 
	\begin{aligned}
	y & = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon \\
	y & = (\iota,x_1,x_2,x_3)\beta + \epsilon \\
	y & = X\beta + \epsilon
	\end{aligned}
$$

where \\( \epsilon_i \sim \mathcal{N}(0,1)\\), \\( \iota \\) is a vector of ones (_i.e._ the constant), and \\( \beta = (1,0.5,1,-1)'\\) is the \\(4\times 1\\) vector of parameters.


We want to generate data following this regression model with _k=3_ regressors (plus the constant) and _n=100_ observations. We generate a matrix _X_ of dimensions \\(n\times k\\) filled with random normal values. For that we need the following _R_ functions:

* _matrix()_ to generate a matrix.
* _rnrom()_ to generate \\(\mathcal{N}(0,1)\\) random numbers.
* _dim()_ to print the dimensions of a matrix.


{% highlight r %}
# Choosing the seed
set.seed(666)

# Setting up the number of variables and observations:
k <- 3
n <- 100

# Creating the matrix of regressors:
X <- matrix(rnorm(k * n), ncol = k, nrow = n)

# Creating the vector of parameters beta:
beta <- matrix(c(0.5, 1, -1), ncol = 1, nrow = 3)

# printing the dimensions of X:
dim(X)
{% endhighlight %}



{% highlight text %}
## [1] 100   3
{% endhighlight %}


 We can now generate the variable _y_ using the matrix multiplication operator _%*%_


{% highlight r %}
# First I generate the innovations:
eps <- rnorm(n)

# Then y:
y <- 1 + X %*% beta + eps

# And check that the dimensions make sense
dim(y)
{% endhighlight %}



{% highlight text %}
## [1] 100   1
{% endhighlight %}



### Estimating \\(\beta\\) by OLS

The generic function to estimate OLS in *R* is called _lm()_, notice that it automatically adds a constant to our model.


{% highlight r %}
ols.lm <- lm(y ~ X)
summary(ols.lm)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.905 -0.626 -0.110  0.675  2.171 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   0.9873     0.0987   10.00  < 2e-16 ***
## X1            0.4840     0.0972    4.98  2.8e-06 ***
## X2            1.2524     0.0881   14.21  < 2e-16 ***
## X3           -0.9078     0.0986   -9.20  7.6e-15 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.979 on 96 degrees of freedom
## Multiple R-squared:  0.78,	Adjusted R-squared:  0.773 
## F-statistic:  113 on 3 and 96 DF,  p-value: <2e-16
{% endhighlight %}


The summary of the _lm()_ function shows many statistics that we will learn to understand throughout the course. If you try _plot(ols.lm)_ you will also get a number of useful plots, but... This is all too easy! 


It is more fun to code the OLS estimator ourselves. Remember the formula for the OLS estimator \\(\hat{\beta}\\) of \\(\beta\\) in matrix form: \\(\hat{\beta}=\left(X'X\right)^{-1}X'y\\). For that we need only two functions: 

* The transpose function: _t()_
* The matrix inversion function _solve()_

We also need to remember adding a constant to X using the _cbind()_ function. 



{% highlight r %}
# Adding the constant
X <- cbind(1, X)

# Estimating beta hat
beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y
print(beta.hat)
{% endhighlight %}



{% highlight text %}
##         [,1]
## [1,]  0.9873
## [2,]  0.4840
## [3,]  1.2524
## [4,] -0.9078
{% endhighlight %}


The estimates of \\(\hat{\beta}\\) are the same using both methods, that is nice. 



 > ## Question:
 > Compute the standard errors of the residuals and of the OLS estimator. To do this, you will need to:
 > 
 > 1. Compute the OLS residuals:
  * compute the fit \\( \hat{y}=X\hat{\beta} \\),
  * compute the residuals \\( \hat{\epsilon} = y - \hat{y} \\). This should be a vector of the same length as _y_.
 > 2. Compute \\(  \hat{\sigma}^2=\frac{\hat{\epsilon}'\hat{\epsilon}}{n-(k+1)}  \\), to get the standard error you need to use the _sqrt()_ function on \\(\hat{\sigma}^2\\)
 > 3. Get the individual standard errors:
  * Get the diagonal of \\((X'X)^{-1}\\) using _diag()_.
  * Scale its square-root by \\(\hat{\sigma}\\). You should get a vector of standard errors of the same length as _beta.hat_.


 **To compare the output on your machine with the one in this post, make sure you have set the same seed as I have.**


{% highlight r %}
# Computing the residuals
y.hat <- X %*% beta.hat
e.hat <- y - y.hat

# Residual standard error
sig2 <- (t(e.hat) %*% e.hat)/(n - (k + 1))
res.se <- sqrt(sig2)
print(res.se)
{% endhighlight %}



{% highlight text %}
##        [,1]
## [1,] 0.9785
{% endhighlight %}



{% highlight r %}

# Parameter std. err.
diag.xx <- diag(solve(t(X) %*% X))
beta.se <- res.se * sqrt(diag.xx)
print(beta.se)
{% endhighlight %}



{% highlight text %}
## [1] 0.09869 0.09717 0.08813 0.09863
{% endhighlight %}



### Visualizing the results. 

Let evaluate the quality of our model graphically. To do that we can look at a scatter plot of the observed values and the residuals.


{% highlight r %}
# Creating a data frame with the resituals, and the observed and fitted
# values.
ols.man <- data.frame(residuals = e.hat, fitted = y.hat, observed = y)
# Loading a plotting library.
require(ggplot2)
sqplot <- ggplot(ols.man, aes(y = observed, x = residuals)) + geom_point()
{% endhighlight %}



{% highlight r %}
print(sqplot)
{% endhighlight %}

![center](/figs/2014-01-27-IE14-W1_rmd/fig-sq.png) 


we can also look at the density of our residuals.


{% highlight r %}
densplot <- ggplot(ols.man, aes(x = residuals)) + geom_density()
print(densplot)
{% endhighlight %}

![center](/figs/2014-01-27-IE14-W1_rmd/fig-dens.png) 


 > ## Question:
 > 
 > Does this look like a Gaussian density to you? What happens when you re-run the code increasing the sample size _n_ to 1000?

