---
layout: post
title: 'Week 1: Multiple Regressions and Matrix Notation.' 
description: "Week 1: Multiple Regressions and Matrix Notation."
category: ie
tags: [IE14, teaching, week1]
comments: true  
published: true
status: process
---



Regressions with multiple variables and matrix notation
========================================================

This week we discussed:

 * Regressions with multiple variables.
 * The OLS in natrix notation.
 * Properties of the OLS  estimator.
 * Omitted variable bias.

This post will focus on:

 * Generating artificial data.
 * Use matrices in the code.
 * Estimate the OLS using built in functions
 * Manually estimate the OLS.
 * Calculate the parameters standard errors.
 * Plot the density of the residuals.
 * Load the wage data and compute regressions.


You can get help on any function by typing _?FunctionName_ in the _R_ console, try _?dim_ for example. 

### Generating data following a regression model with multiple variables
Let us assume the response variable _y_ follows a regression model with 3 explanatory variables:

$$ 
	\begin{aligned}
	y & = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon \\
	y & = (\iota,x_1,x_2,x_3)\beta + \epsilon \\
	y & = X\beta + \epsilon
	\end{aligned}
$$

where \\( \epsilon_i \sim \mathcal{N}(0,1)\\), \\( \iota \\) is a vector of ones (_i.e._ the constant), and \\( \beta = (1,0.5,1,-1)'\\) is the \\(4\times 1\\) vector of parameters.


We want to generate data following this regression model with _k=3_ regressors (plus the constant) and _n=100_ observations. We generate a matrix _X_ of dimensions \\(n\times k\\) filled with random normal values. For that we need the following _R_ functions:

* _matrix()_ to generate a matrix.
* _rnrom()_ to generate \\(\mathcal{N}(0,1)\\) random numbers.
* _dim()_ to print the dimensions of a matrix.


{% highlight r %}
# Choosing the seed
set.seed(666)

# Setting up the number of variables and observations:
k <- 3
n <- 100

# Creating the matrix of regressors:
X <- matrix(rnorm(k * n), ncol = k, nrow = n)

# Creating the vector of parameters beta:
beta <- matrix(c(0.5, 1, -1), ncol = 1, nrow = 3)

# printing the dimensions of X:
dim(X)
{% endhighlight %}



{% highlight text %}
## [1] 100   3
{% endhighlight %}


 We can now generate the variable _y_ using the matrix multiplication operator _%*%_


{% highlight r %}
# First I generate the innovations:
eps <- rnorm(n)

# Then y:
y <- 1 + X %*% beta + eps

# And check that the dimensions make sense
dim(y)
{% endhighlight %}



{% highlight text %}
## [1] 100   1
{% endhighlight %}



### Estimating \\(\beta\\) by OLS

The generic function to estimate OLS in *R* is called _lm()_, notice that it automatically adds a constant to our model.


{% highlight r %}
# estimate the model.
ols.lm <- lm(y ~ X)
# print the results.
summary(ols.lm)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.905 -0.626 -0.110  0.675  2.171 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   0.9873     0.0987   10.00  < 2e-16 ***
## X1            0.4840     0.0972    4.98  2.8e-06 ***
## X2            1.2524     0.0881   14.21  < 2e-16 ***
## X3           -0.9078     0.0986   -9.20  7.6e-15 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.979 on 96 degrees of freedom
## Multiple R-squared:  0.78,	Adjusted R-squared:  0.773 
## F-statistic:  113 on 3 and 96 DF,  p-value: <2e-16
{% endhighlight %}


The summary of the _lm()_ function shows many statistics that we will learn to understand throughout the course. If you try _plot(ols.lm)_ you will also get a number of useful plots, but... This is all too easy! 


It is more fun to code the OLS estimator ourselves. Remember the formula for the OLS estimator \\(\hat{\beta}\\) of \\(\beta\\) in matrix form: \\(\hat{\beta}=\left(X'X\right)^{-1}X'y\\). For that we need only two functions: 

* The transpose function: _t()_
* The matrix inversion function _solve()_

We also need to remember adding a constant to X using the _cbind()_ function. 



{% highlight r %}
# Adding the constant
X <- cbind(1, X)

# Estimating beta hat
beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y
print(beta.hat)
{% endhighlight %}



{% highlight text %}
##         [,1]
## [1,]  0.9873
## [2,]  0.4840
## [3,]  1.2524
## [4,] -0.9078
{% endhighlight %}


The estimates of \\(\hat{\beta}\\) are the same using both methods, that is nice. 



## Question:
Compute the standard errors of the residuals and of the OLS estimator. To do this, you will need to:
  
 1. Compute the OLS residuals:
  * compute the fit \\( \hat{y}=X\hat{\beta} \\),
  * compute the residuals \\( \hat{\epsilon} = y - \hat{y} \\). This should be a vector of the same length as _y_.
 2. Compute \\(  \hat{\sigma}^2=\frac{\hat{\epsilon}'\hat{\epsilon}}{n-(k+1)}  \\), to get the standard error you need to use the _sqrt()_ function on \\(\hat{\sigma}^2\\)
 3. Get the individual standard errors:
  * Get the diagonal of \\((X'X)^{-1}\\) using _diag()_.
  * Scale its square-root by \\(\hat{\sigma}\\). You should get a vector of standard errors of the same length as _beta.hat_.


 **To compare the output on your machine with the one in this post, make sure you have set the same seed as I have.**


{% highlight r %}
# Computing the residuals
y.hat <- X %*% beta.hat
e.hat <- y - y.hat

# Residual standard error
sig2 <- (t(e.hat) %*% e.hat)/(n - (k + 1))
res.se <- sqrt(sig2)
cat("\n residual standard errors:", res.se, sep = "")
{% endhighlight %}



{% highlight text %}
## 
##  residual standard errors:0.9785
{% endhighlight %}



{% highlight r %}

# Parameter std. err.
diag.xx <- diag(solve(t(X) %*% X))
beta.se <- res.se * sqrt(diag.xx)
cat("\n Parameters standard errors:", beta.se, sep = "")
{% endhighlight %}



{% highlight text %}
## 
##  Parameters standard errors:0.098690.097170.088130.09863
{% endhighlight %}



### Visualizing the results. 

Let evaluate the quality of our model graphically. To do that we can look at a scatter plot of the observed values and the residuals.


{% highlight r %}
# Creating a data frame with the resituals, and the observed and fitted
# values.
ols.man <- data.frame(residuals = e.hat, fitted = y.hat, observed = y)
# Loading a plotting library.
require(ggplot2)
sqplot <- ggplot(ols.man, aes(y = observed, x = residuals)) + geom_point()
{% endhighlight %}



{% highlight r %}
print(sqplot)
{% endhighlight %}

![center](/figs/2014-01-27-IE14-W1_rmd/fig-sq.png) 


we can also look at the density of our residuals.


{% highlight r %}
densplot <- ggplot(ols.man, aes(x = residuals)) + geom_density()
print(densplot)
{% endhighlight %}

![center](/figs/2014-01-27-IE14-W1_rmd/fig-dens.png) 


## Question:
  
Does this look like a Gaussian density to you? What happens when you re-run the code increasing the sample size _n_ to 1000?

### Multiple regression with the wage data. 

First we load the data and check that it looks as expected.

{% highlight r %}
# Loading the data.
wage <- read.csv("../data/wages.csv")
# Printing the first rows of wage.
head(wage)
{% endhighlight %}



{% highlight text %}
##   X EXPER MALE SCHOOL  WAGE
## 1 1     9    0     13 6.315
## 2 2    12    0     12 5.480
## 3 3    11    0     11 3.642
## 4 4     9    0     14 4.593
## 5 5     8    0     14 2.418
## 6 6     9    0     14 2.094
{% endhighlight %}


Now we can regress _WAGE_ on _SCHOOL_ and _EXPER_:

{% highlight r %}
# regression of wage on school and exper.
mod1 <- lm(WAGE ~ SCHOOL + EXPER, data = wage)
# printing the results of the regression.
summary(mod1)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = WAGE ~ SCHOOL + EXPER, data = wage)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -6.88  -1.99  -0.52   1.39  34.91 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  -2.4767     0.4700   -5.27  1.5e-07 ***
## SCHOOL        0.5992     0.0334   17.94  < 2e-16 ***
## EXPER         0.1573     0.0242    6.51  8.9e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.12 on 3291 degrees of freedom
## Multiple R-squared:  0.0915,	Adjusted R-squared:  0.0909 
## F-statistic:  166 on 2 and 3291 DF,  p-value: <2e-16
{% endhighlight %}


## Question:
 Try estimating other models using different combination of variables.  

### Transformations of the data

Maybe the number of years at school does not enter linearily in the model. We can expand the model with a quadratic term:

{% highlight r %}
# Create a variable with the square of the number of school years.
wage$SCHOOL2 <- wage$SCHOOL^2
# Estimating the model and printing the results.
mod2 <- lm(WAGE ~ SCHOOL + SCHOOL2 + EXPER, data = wage)
summary(mod2)
{% endhighlight %}



{% highlight text %}
## 
## Call:
## lm(formula = WAGE ~ SCHOOL + SCHOOL2 + EXPER, data = wage)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -7.33  -1.96  -0.51   1.38  34.97 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)   3.1149     1.5475    2.01  0.04421 *  
## SCHOOL       -0.3840     0.2614   -1.47  0.14201    
## SCHOOL2       0.0433     0.0114    3.79  0.00015 ***
## EXPER         0.1409     0.0245    5.75  9.8e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.11 on 3290 degrees of freedom
## Multiple R-squared:  0.0954,	Adjusted R-squared:  0.0946 
## F-statistic:  116 on 3 and 3290 DF,  p-value: <2e-16
{% endhighlight %}



## Question:
 1. Interpret the results above.
 2. Maybe the number of years of schooling has a different effect for males and for females. How would you investigate this?  
