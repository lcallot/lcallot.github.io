---
layout: post
title: 'Week 2-3: R squared, Testing, and Monte Carlo.' 
description: "Week 2-3: R squared, Testing, and Monte Carlo."
category: ie
tags: [IE14, teaching, week23]
comments: true  
published: true
status: process
---



Regressions with multiple variables and matrix notation
========================================================

These last 2 weeks we discussed:

 * The R squared.
 * Test for linear restrictions.
 * Asymptotic properties of the OLS  estimator.
 * Omitted variable bias.

This post will focus on:

 * Estimate empirical distributions of the OLS estimator.
 * Investigate the finite sample size and power of tests. 


### R Squared with increasing number of regressors

Below the code used to generate the R squared plots in the homework slides of Week 2
 
 

{% highlight r %}
# loading the plotting library
library("ggplot2")

# setting the seed
set.seed(42)

# Number of observations
n <- 100

# y is just white noise
y <- matrix(rnorm(n), ncol = 1)

# a vector with the number of control variables
k.all <- seq(from = 0, to = n, by = round(n/50, 0))

# storage initialization
r2 <- matrix(NA, ncol = length(k.all), nrow = 2)
colnames(r2) <- k.all
rownames(r2) <- c("R2", "adjusted R2")

# counter
i <- 1
# looping over the numbers of controls.
for (k in k.all) {
    # creating the controls with a constant
    
    X <- cbind(1, matrix(rnorm(n * (ifelse(k == 0, 0, k - 1))), nrow = n, ncol = ifelse(k == 
        0, 0, k - 1)))
    
    # regression + computation of summary statistics
    slm <- summary(lm(y ~ X))
    
    # Storage of the resuts
    r2[1, i] <- slm$r.squared
    r2[2, i] <- slm$adj.r.squared
    
    # increment the counter
    i <- i + 1
}
{% endhighlight %}



{% highlight r %}
# plotting the results:

# preparing the data
mr2 <- melt(r2)
names(mr2)[1:2] <- c("mesure", "k")

# plotting
gr2 <- ggplot(mr2, aes(x = k, y = value, colour = mesure)) + geom_line()
print(gr2)
{% endhighlight %}



{% highlight text %}
## Warning: Removed 1 rows containing missing values (geom_path).
{% endhighlight %}

![center](/figs/2014-02-17-IE14-W23_rmd/plotr2.png) 




## Finite sample distribution of the OLS estimator

\\( \hat{\beta} \\) is normaly distruted if the residuals are Gaussian, but it is asymptotically normal under weaker assumptions. How good is the gaussian approximation? We evaluate this using a Monte Carlo experiment. 




{% highlight r %}
# Let's create a function returning the mc estimates of beta Inputs: - n the
# number of observations - mc the number of monte carlo iterations - k the
# number of control variables - tdf the degrees of freedom of the t
# distribution Output:
mcbeta <- function(n, mc, k, tdf) {
    
    # We use the same set of controls for every experiment
    X <- matrix(rnorm(n * k), nrow = n, ncol = k)
    
    # storage matrix
    bN <- matrix(NA, nrow = mc, ncol = k)
    bT <- matrix(NA, nrow = mc, ncol = k)
    
    for (m in 1:mc) {
        epsN <- rnorm(n, sd = sqrt(tdf/(tdf - 2)))  # standard normal innovations
        epsT <- rt(n, tdf)  #t(1) innovations
        
        # the true beta is 1 in both cases
        yN <- X + epsN
        yT <- X + epsT
        
        # Storing the estimated beta, discarding the constant
        bN[m, ] <- coef(lm(yN ~ X))[-1]
        bT[m, ] <- coef(lm(yT ~ X))[-1]
        
        # returning a list with the results
    }
    return(list(Normal = bN, Student = bT))
}
{% endhighlight %}




Now let's run a first experiment with a very small sample of n=10 and plot the results as two overlapping histograms:


{% highlight r %}
# nbr of obs
n <- 10
# nbr of Monte Carlo iterations
mc <- 1000
# nbr of controls
k <- 1
# degrees of freedom
tdf <- 3

mc10 <- mcbeta(n, mc, k, tdf)
mc10 <- melt(mc10)

colnames(mc10) <- c("iteration", "a", "value", "residuals")

gg.mc10 <- ggplot(mc10, aes(x = value, fill = residuals)) + geom_histogram(binwidth = 0.1, 
    alpha = 0.5, position = "identity")
print(gg.mc10)
{% endhighlight %}

![center](/figs/2014-02-17-IE14-W23_rmd/mc10.png) 


\\(\hat{\beta} \\) is estimated to be somewhere between -1 and 4 when the innovations are _t(3)_, while when they are _N(0,3)_ the range is around [0,2.5]. The _t_ distributed innovations produce many extreme estimates of \\( \beta \\). Because we seek to minimize the squared residuals, OLS is very sensitive to extreme observations.

What happens when we increase the number of observations to 100?  



{% highlight r %}
# nbr of obs
n <- 100
mc100 <- mcbeta(n, mc, k, tdf)
mc100 <- melt(mc100)

colnames(mc100) <- c("iteration", "a", "value", "residuals")

gg.mc100 <- ggplot(mc100, aes(x = value, fill = residuals)) + geom_histogram(binwidth = 0.01, 
    alpha = 0.5, position = "identity")
print(gg.mc100)
{% endhighlight %}

![center](/figs/2014-02-17-IE14-W23_rmd/mc100.png) 


Both distributions are closer to the true value, but the _t_ distribution still produces many more outliers. Now with 1000 observations:



{% highlight r %}
# nbr of obs
n <- 1000
mc1000 <- mcbeta(n, mc, k, tdf)
mc1000 <- melt(mc1000)

colnames(mc1000) <- c("iteration", "a", "value", "residuals")

gg.mc1000 <- ggplot(mc1000, aes(x = value, fill = residuals)) + geom_histogram(binwidth = 0.005, 
    alpha = 0.5, position = "identity")
print(gg.mc1000)
{% endhighlight %}

![center](/figs/2014-02-17-IE14-W23_rmd/mc1000.png) 


With 1000 observations the distribution the two distributions are similar, in this case the normal approximation is good but we still have more extreme observations with the _t_ innovations.

Let's keep 1000 observation but increase the number of degrees of freedoms of the _t_ distributions so that it generates fewer extreme innovations:


{% highlight r %}
# nbr of obs
tdf <- 10

mcdf10 <- mcbeta(n, mc, k, tdf)
mcdf10 <- melt(mcdf10)

colnames(mcdf10) <- c("iteration", "a", "value", "residuals")

gg.mcdf10 <- ggplot(mcdf10, aes(x = value, fill = residuals)) + geom_histogram(binwidth = 0.005, 
    alpha = 0.5, position = "identity")
print(gg.mcdf10)
{% endhighlight %}

![center](/figs/2014-02-17-IE14-W23_rmd/mcdf10.png) 


With 10 degrees of freedom, the two distributions are almost overlapping, the normal approximation is a good one in this case.

## Question: 
Can you setup a similar experiment to evaluate the size and the power of the _t_ test?
