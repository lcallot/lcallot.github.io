---
layout: post
title: 'Week 3:  Monte Carlos.' 
description: "Week 2-3: Monte Carlos."
category: ie15
tags: [IE15, teaching, week3]
comments: true  
published: true
status: process
---

This week we conduct some simulation experiments to investigate:

1. The finite sample distribution of the OLS estimator in the case where the innovations are Gaussian or Student distributed. 
2. The size and power of the t-test for varying sample sizes. 



{% highlight r %}
# Loading the libraries. 
library('ggplot2')
library('reshape2')
library('plot3D')

set.seed(2512800)
{% endhighlight %}


## Finite sample distribution of the OLS estimator

In this section we conduct a series of simulation experiments to investigate the finite sample distribution of the ols estimator. Here are the steps we take:

1. We build a function to perform the simulations. This function returns many estimates of \\( \hat{\beta} \\) when the data is generated with either Gaussian or Student distributed residuals. 
2. We compare the empirical distributions of the estimators by plotting two overlapping histograms, one for each distribution.
3. We repeat the experiments with different numbers of observations and degrees of freedom. 


The simulation function computes many ols estimators on generated data where the residuals are either standard normal of t-distributed. The function takes as inputs the number of observations, of simulations, of control variables, and the number of degrees of freedom for the t-distribution of the innovations. It returns a list with the estimated parameters for the Gaussian and the t-residuals. For simplicity the true value of all entries of \\( \hat{\beta} \\) are equal to 1. In order to have the same variance for the Gaussian and Student residuals, we set the variance of the Gaussian innovations to \\( \frac{tdf}{(tdf-2)} \\) where \\(tdf\\) is the number of degrees of freedom of the Student innovations. 


{% highlight r %}
#Let's create a function returning the mc estimates of beta 
# Inputs: - n the number of observations
#         - mc the number of monte carlo iterations
#         - k the number of control variables
#         - tdf the degrees of freedom of the t distribution 
# Output: - a list with two matrices of betaN and betaT.
mcbeta <- function(n,mc,k,tdf){
  

  #storage matrix
  bN <- matrix(NA,nrow=mc,ncol=k)
  bT <- matrix(NA,nrow=mc,ncol=k)
  
  for(m in 1:mc){
  	# Set of controls
    X <- matrix(rnorm(n*k),nrow=n,ncol=k)
  
    epsN <- rnorm(n,sd=sqrt(tdf/(tdf-2))) # Gaussian innovations
    epsT <- rt(n,tdf) #Student innovations
    
    # the true beta is 1 in both cases
    yN <- X + epsN
    yT <- X + epsT
    
    # Storing the estimated beta, discarding the constant
    bN[m,] <- coef(lm(yN ~ X))[-1]
    bT[m,] <- coef(lm(yT ~ X))[-1]
    
    # returning a list with the results
  }
  return(list('Normal'=bN,'Student'=bT))
}
{% endhighlight %}



Now let's run a first experiment with a very small sample of n=10 and plot the results as two overlapping histograms:


{% highlight r %}
#nbr of obs
n <- 10
#nbr of Monte Carlo iterations
mc <- 1000
# nbr of controls
k <- 1
# degrees of freedom 
tdf <- 3

mc10 <- mcbeta(n,mc,k,tdf)
mc10 <- melt(mc10)

colnames(mc10) <- c('iteration','a','value','residuals')

gg.mc10 <- ggplot(mc10,aes(x=value,fill=residuals)) + geom_histogram(binwidth=.1, alpha=.5, position='identity')
print(gg.mc10)
{% endhighlight %}

![center](/figs/2015-02-15-IE14-W3.rmd/mc10.png) 

\\(\hat{\beta} \\) is estimated to be somewhere between -2 and 3 when the innovations are _t(3)_, while when they are _N(0,3)_ the range is around [0,2.5]. The _t_ distributed innovations produce many extreme estimates of \\( \beta \\). Because we seek to minimize the squared residuals, OLS is very sensitive to extreme observations.

What happens when we increase the number of observations to 100?  



{% highlight r %}
#nbr of obs
n <- 100
# Estimation
mc100 <- mcbeta(n,mc,k,tdf)
mc100 <- melt(mc100)

colnames(mc100) <- c('iteration','a','value','residuals')

gg.mc100 <- ggplot(mc100,aes(x=value,fill=residuals)) + geom_histogram(binwidth=.01, alpha=.5, position='identity')
print(gg.mc100)
{% endhighlight %}

![center](/figs/2015-02-15-IE14-W3.rmd/mc100.png) 

Both distributions are closer to the true value, but the _t_ distribution still produces many more outliers. Now with 1000 observations:



{% highlight r %}
#nbr of obs
n <- 1000
# Estimation
mc1000 <- mcbeta(n,mc,k,tdf)
mc1000 <- melt(mc1000)

colnames(mc1000) <- c('iteration','a','value','residuals')

gg.mc1000 <- ggplot(mc1000,aes(x=value,fill=residuals)) + geom_histogram(binwidth=.005, alpha=.5, position='identity')
print(gg.mc1000)
{% endhighlight %}

![center](/figs/2015-02-15-IE14-W3.rmd/mc1000.png) 

With 1000 observations the distribution the two distributions are similar, in this case the normal approximation is good but we still have more extreme observations with the _t_ innovations.

Let's keep 1000 observation but increase the number of degrees of freedoms of the _t_ distributions so that it generates fewer extreme innovations:


{% highlight r %}
#nbr of obs
tdf <- 10
# Estimation
mcdf10 <- mcbeta(n,mc,k,tdf)
mcdf10 <- melt(mcdf10)

colnames(mcdf10) <- c('iteration','a','value','residuals')

gg.mcdf10 <- ggplot(mcdf10,aes(x=value,fill=residuals)) + geom_histogram(binwidth=.005, alpha=.5, position='identity')
print(gg.mcdf10)
{% endhighlight %}

![center](/figs/2015-02-15-IE14-W3.rmd/mcdf10.png) 

With 10 degrees of freedom, the two distributions are almost overlapping, the normal approximation is a good one in this case.

## Size and power of the t-test. 



{% highlight r %}
#Let's create a function returning the mc estimates of beta 
# Inputs: - n the number of observations
#         - mc the number of monte carlo iterations
#         - k the number of control variables

# Output: 
mcsp <- function(n,mc){
  
  # vector of values b for which we test H0: \hat\beta = b
  b <- seq(0,1,0.01)
  #storage matrix
  tstats <- matrix(NA,nrow=mc,ncol=length(b))
  colnames(tstats) <- b
  # Looping over the number of iteratons 
  for(m in 1:mc){
    # We use a new set of controls for every experiment
    X <- matrix(rnorm(n),nrow=n,ncol=1)
    # standard normal innovations
    eps <- rnorm(n,sd=1)
    # the true beta is 1 
    y <- X + eps    
    # Storing the estimated beta with std.err
    smy <- summary(lm(y ~ X))$coefficients
    # Computing the vector of t-statistcs
    tstats[m,] <- (smy[2,1] - b)/smy[2,2]
    
  }
# returning the vector of t-statistics.
  return(tstats)
}
{% endhighlight %}

In this first experiment we set n=100 and compute a 95\% t-test for the hypothesis \\(\mathcal{H}_0:\hat\beta_i = b\\) with b going from 0 to 1 (the true value) by steps of 0.01. We plot below the rejection frequency for each value of b.  


{% highlight r %}
#nbr of obs
n <- 50
#nbr of Monte Carlo iterations
mc <- 1000
# Critical value for the t-test
cv <- 1.96
# running the experiments
sp100 <- mcsp(n,mc)

msp100 <- melt(colMeans(abs(sp100>cv)))
msp100 <- cbind(as.numeric(colnames(sp100)),msp100)
colnames(msp100) <- c('b','value')

gg.msp100 <- ggplot(msp100,aes(x=b,y=value)) + geom_line() 
print(gg.msp100)
{% endhighlight %}

![center](/figs/2015-02-15-IE14-W3.rmd/ps100.png) 

We now repeat the same experiment as before for varying sample size. Each ribbon in the plot shows the rejection frequency for every value of b (y axis) at a given sample size (x axis). So the size of the test is the rejection frequency at b=1, it is always close to 0.05 the nominal value, and the rejection frequencies for b>1 are estimates of the power of the test for a given sample sizes. The power always starts close to 0.05 and climbs towards 1 while we move away from the true hypothesis. The climb is faster as the sample size increases: the more information the more often we reject false hypotheses.   



{% highlight r %}
#nbr of obs
b <- seq(0,1,0.01)
vobs <- c(5,10,15,20,25,30,35,40,45,50,60,70,80,90,100)
# Storage
sp <- sp2 <- matrix(NA,nrow=length(vobs),ncol=length(b))
#nbr of Monte Carlo iterations
mc <- 1000
# Critical value for the t-test
cv <- 1.96
cv2<- qt(0.975,df=vobs-2)
# Looping over the sample sizes. 
ncount <- 0
for(n in vobs){
	ncount <- ncount + 1
	# running the experiments
	sp100 <- mcsp(n,mc)
	sp[ncount,] <- colMeans(abs(sp100>cv))
	sp2[ncount,] <- colMeans(abs(sp100>cv2[ncount]))
	}
colnames(sp2) <- colnames(sp) <- b
rownames(sp2) <- rownames(sp) <- vobs


ribbon3D(x=vobs,y=b,z =sp, along ='y', clab = 'rejection frequency', shade = 0.5, bty = "g", ticktype = "detailed",phi=20,theta=-110)
{% endhighlight %}

![center](/figs/2015-02-15-IE14-W3.rmd/sp.png) 

Note that for the previous plot we used the quantile from the Gaussian distributions (i.e. from the asymptotic distribution of \\(\mathcal{H}_0\\)). In the plot below we use the exact Student critical values.  


{% highlight r %}
ribbon3D(x=vobs,y=b,z =sp2, along ='y', clab = 'rejection frequency', shade = 0.5, bty = "g", ticktype = "detailed",phi=20,theta=-110)
{% endhighlight %}

![center](/figs/2015-02-15-IE14-W3.rmd/sp2.png) 

