<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Laurent's Research Page</title>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="/feed.xml" />
<link rel="alternate" type="text/html" href="" />
<updated>2015-03-05T14:56:17+01:00</updated>
<id>/</id>
<author>
  <name>Laurent Callot</name>
  <uri>/</uri>
  <email>lcallot@gmail.com</email>
</author>


<entry>
  <title type="html"><![CDATA[Week 5:  Maximum Likelihood.]]></title>
  <link>/ie15/IE15-W5</link>
  <id>/ie15/IE15-W5</id>
  <published>2015-03-03T00:00:00+01:00</published>
  <updated>2015-03-03T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;This week we take a look at some numerical issues with maximum likelihood estimation.
  1. We take a look at the effect of the choice of starting value on the outcome of the a BFGS optmization.
  2. We look at the behaviour of the MLE of the parameter of a Bernoulli distribution when the sample size increases. We also look at the behaviour of test statistics.&lt;/p&gt;

&lt;h2 id=&quot;local-extremum-and-starting-values&quot;&gt;Local extremum and starting values.&lt;/h2&gt;

&lt;p&gt;First we are going to look at the problems with functions having several local minima, and how the choice of starting value is critical in that case. We consider a function \( f(x) = -cos(x) - exp(-x^2) \) which has a global minimum at 0 and several local minima close to the points when \(x\) is a multiple of \( 2\pi \).&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Firs we load some libraries.
library('ggplot2')
library('reshape2')&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Define the likelihood function:
f &amp;lt;- function(x){ -cos(x)/(1+abs(x)) - exp(-x^2) }

# Generate a set of (x,f(x)) for plotting
x &amp;lt;- seq(-20,20,0.01)
y &amp;lt;- f(x)

# plotting 
xy &amp;lt;- data.frame('x'=x,'y'=y)
gxy &amp;lt;- ggplot(xy,aes(x=x,y=y)) + geom_line() + theme_bw()
print(gxy)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-03-03-IE15-W5.rmd/local_ll.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are now going to try to numerically locate the value of \(x\) that minimizes \(f(.)\) for a set of starting values. That is, we try one starting value after another and see where the minimum is found, using a BFGS algorithm.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Define a set of starting values. 
par_init &amp;lt;- seq(-20,20,0.5)

# storage
pmat &amp;lt;- matrix(NA,nrow = length(par_init), ncol=2)
pmat[,1] &amp;lt;- par_init
count &amp;lt;- 0

# Looping over the starting values. 
for(par in par_init){
	count &amp;lt;- count + 1
	bf &amp;lt;- optim(par=par,fn=f,method='BFGS')
	pmat[count,2] &amp;lt;- bf$par
	}

# preparing the data
pmat &amp;lt;- cbind(pmat,f(pmat[,1]),f(pmat[,2]))
colnames(pmat) &amp;lt;- c('x_init','x_opt','y_init','y_opt')

mi &amp;lt;- data.frame('x'=pmat[,1],'y'=pmat[,3],'iter'=1:length(par_init))
mi$type &amp;lt;- 'initial'
mo &amp;lt;- data.frame('x'=pmat[,2],'y'=pmat[,4],'iter'=1:length(par_init))
mo$type &amp;lt;- 'optimal'

mp &amp;lt;- rbind(mi,mo)
mp$iter &amp;lt;- as.factor(mp$iter)	
	
#plotting
gp &amp;lt;- ggplot()+ geom_line(data=mp,aes(x=x,y=y,group=iter),colour='gray80') + geom_line(data=xy,aes(x=x,y=y)) + theme_bw() + geom_point(data=mp,aes(x=x,y=y,colour=type),size=3) + scale_x_continuous(limits = c(-20, 20))
print(gp)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-03-03-IE15-W5.rmd/local_bfgs.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The red-ish dots show the starting values, we try every value from -20 to 20 by steps of 0.5. the blue dots represtent the optimal value returned by BFGS. A grey line connects each inital value to its corresponding optimal value.&lt;/p&gt;

&lt;p&gt;There is far fewer blue than red dots, for many starting values the algorithm converges at the same point. The 7 blue dots in the figure correspond to the 6 local minima of the functions (x close to being a multiple of \( 2\pi \)) and the global miminum at \(x=0\).&lt;/p&gt;

&lt;p&gt;The most noticeable of the lines joining an initial value to its convergence value are those corresponding to pairs where we start far from the optimal and end up on it or the reverse: inital values close to the optimal value with convergence values far from 0.&lt;/p&gt;

&lt;h2 id=&quot;mle-and-tests-for-the-parameter-of-a-bernoulli-distribution-with-increasing-sample-size&quot;&gt;MLE and tests for the parameter of a Bernoulli distribution with increasing sample size.&lt;/h2&gt;

&lt;p&gt;This code is meant to allow you to replicate the plots at the end of the slides of week 5. You will not be able to replicate the exat plots since I forgot to fix the seed when generating them. Rookie mistake.&lt;/p&gt;

&lt;p&gt;First we create the function that will compute the LR, Wald, and LM test statistics for \(\mathcal H_0:p=p_0\) given a vector of realisations of the bernoulli random variable and an hypothesis to test.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Function to compute the 3 tests.
# Input: the data vector X, the null hypothesis p0
# output: a vector of length 3 with the 3 test statistics
Ltest&amp;lt;-function(X,p0)
{
	iT	&amp;lt;- length(X)
	LR	&amp;lt;-2*((sum(X)*log(mean(X))+(length(X)-sum(X))*log(1-mean(X)))-(sum(X)*log(p0)+(length(X)-sum(X))*log(1-p0)))

	LM	&amp;lt;-(iT^2)*(((mean(X)/p0)-(1-mean(X))/(1-p0))^2)*((p0*(1-p0))/iT)

	W	&amp;lt;-iT*((mean(X)-p0)^2)/(mean(X)*(1-mean(X)))

return(cbind(LR,LM,W))
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next we create the main loop function. At each iterations the sample is increased by one observation. We estimate \(\hat p \) and its variance and and call the previous function to compute the 3 test statistics. This function then formats the output for plotting.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;bernoulli &amp;lt;- function(p,p0,n){
# Initialization
X	&amp;lt;-NULL
hp	&amp;lt;-NULL
LLtest	&amp;lt;-matrix(NA,nrow=3,ncol=3)
# Estimation with increasing sample. 
for(i in 1:n)
	{
	xi	&amp;lt;-rbinom(1,1,p)	
	X	&amp;lt;-c(X,xi)
	if(i&amp;gt;1)hi	&amp;lt;-cbind(mean(X),mean(X)*(1-mean(X)))
	if(i==1)hi	&amp;lt;-cbind(X,NA)
	hp	&amp;lt;-rbind(hp,hi)
	if(i&amp;gt;3)Lt	&amp;lt;-Ltest(X,p0)
	if(i&amp;gt;3)LLtest	&amp;lt;-rbind(LLtest,Lt)
	}
# formatting
colnames(hp)&amp;lt;-c('p-hat','est var')
mhp&amp;lt;-melt(hp)
# true parameter value and variance.
dfseg&amp;lt;-data.frame(y=c(p,p*(1-p)),Var2=c('p-hat','est var'))
# formatting the test stats
colnames(LLtest)&amp;lt;-c('LR','LM','W')
rownames(LLtest)&amp;lt;-1:n
mLL		&amp;lt;-melt(LLtest)

return(list('mle'=mhp,'trueval'=dfseg,'lltests'=mLL))
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we can run an experiment. All we need to do is choose values for \(p,p_0,n\) and plot the output.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Setting the seed
set.seed(06011655)

# True parameter value, null hypothesis and sample size
# Modify these to replicate the different experiments in the slides. 
p	&amp;lt;-0.5
p0	&amp;lt;-0.5
n&amp;lt;-100

# Critical value of the X2 distrib with 1df for 95% tests
X2cv &amp;lt;- qchisq(0.95,1)

# running the XP
xp &amp;lt;- bernoulli(p,p0,n)

#plotting the parameters
gghp&amp;lt;-ggplot(xp$mle,aes(y=value,x=Var1))+facet_grid(Var2~.)+geom_line()+geom_segment(data=xp$trueval,aes(x=0,xend=n,y=y,yend=y),colour='grey')+theme_bw()
print(gghp)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-03-03-IE15-W5.rmd/bernoulli.xp.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# plotting the test stats
ggtest&amp;lt;-ggplot(xp$lltests,aes(y=value,x=Var1,colour=Var2))+geom_line(size=2)+facet_grid(Var2~.,scales='free_y')+geom_segment(size=1.5,x=0,xend=n,y=X2cv,yend=X2cv)+theme_bw()
print(ggtest)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-03-03-IE15-W5.rmd/bernoulli.xp.plot.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/ie15/IE15-W5&quot;&gt;Week 5:  Maximum Likelihood.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on March 03, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Week 3:  Monte Carlos.]]></title>
  <link>/ie15/IE14-W3</link>
  <id>/ie15/IE14-W3</id>
  <published>2015-02-15T00:00:00+01:00</published>
  <updated>2015-02-15T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;This week we conduct some simulation experiments to investigate:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The finite sample distribution of the OLS estimator in the case where the innovations are Gaussian or Student distributed.&lt;/li&gt;
  &lt;li&gt;The size and power of the t-test for varying sample sizes.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Loading the libraries. 
library('ggplot2')
library('reshape2')
library('plot3D')

set.seed(2512800)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;finite-sample-distribution-of-the-ols-estimator&quot;&gt;Finite sample distribution of the OLS estimator&lt;/h2&gt;

&lt;p&gt;In this section we conduct a series of simulation experiments to investigate the finite sample distribution of the ols estimator. Here are the steps we take:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We build a function to perform the simulations. This function returns many estimates of \( \hat{\beta} \) when the data is generated with either Gaussian or Student distributed residuals.&lt;/li&gt;
  &lt;li&gt;We compare the empirical distributions of the estimators by plotting two overlapping histograms, one for each distribution.&lt;/li&gt;
  &lt;li&gt;We repeat the experiments with different numbers of observations and degrees of freedom.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The simulation function computes many ols estimators on generated data where the residuals are either standard normal of t-distributed. The function takes as inputs the number of observations, of simulations, of control variables, and the number of degrees of freedom for the t-distribution of the innovations. It returns a list with the estimated parameters for the Gaussian and the t-residuals. For simplicity the true value of all entries of \( \hat{\beta} \) are equal to 1. In order to have the same variance for the Gaussian and Student residuals, we set the variance of the Gaussian innovations to \( \frac{tdf}{(tdf-2)} \) where \(tdf\) is the number of degrees of freedom of the Student innovations.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;#Let's create a function returning the mc estimates of beta 
# Inputs: - n the number of observations
#         - mc the number of monte carlo iterations
#         - k the number of control variables
#         - tdf the degrees of freedom of the t distribution 
# Output: - a list with two matrices of betaN and betaT.
mcbeta &amp;lt;- function(n,mc,k,tdf){
  

  #storage matrix
  bN &amp;lt;- matrix(NA,nrow=mc,ncol=k)
  bT &amp;lt;- matrix(NA,nrow=mc,ncol=k)
  
  for(m in 1:mc){
  	# Set of controls
    X &amp;lt;- matrix(rnorm(n*k),nrow=n,ncol=k)
  
    epsN &amp;lt;- rnorm(n,sd=sqrt(tdf/(tdf-2))) # Gaussian innovations
    epsT &amp;lt;- rt(n,tdf) #Student innovations
    
    # the true beta is 1 in both cases
    yN &amp;lt;- X + epsN
    yT &amp;lt;- X + epsT
    
    # Storing the estimated beta, discarding the constant
    bN[m,] &amp;lt;- coef(lm(yN ~ X))[-1]
    bT[m,] &amp;lt;- coef(lm(yT ~ X))[-1]
    
    # returning a list with the results
  }
  return(list('Normal'=bN,'Student'=bT))
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now let’s run a first experiment with a very small sample of n=10 and plot the results as two overlapping histograms:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;#nbr of obs
n &amp;lt;- 10
#nbr of Monte Carlo iterations
mc &amp;lt;- 1000
# nbr of controls
k &amp;lt;- 1
# degrees of freedom 
tdf &amp;lt;- 3

mc10 &amp;lt;- mcbeta(n,mc,k,tdf)
mc10 &amp;lt;- melt(mc10)

colnames(mc10) &amp;lt;- c('iteration','a','value','residuals')

gg.mc10 &amp;lt;- ggplot(mc10,aes(x=value,fill=residuals)) + geom_histogram(binwidth=.1, alpha=.5, position='identity')
print(gg.mc10)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-15-IE14-W3.rmd/mc10.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;\(\hat{\beta} \) is estimated to be somewhere between -2 and 3 when the innovations are &lt;em&gt;t(3)&lt;/em&gt;, while when they are &lt;em&gt;N(0,3)&lt;/em&gt; the range is around [0,2.5]. The &lt;em&gt;t&lt;/em&gt; distributed innovations produce many extreme estimates of \( \beta \). Because we seek to minimize the squared residuals, OLS is very sensitive to extreme observations.&lt;/p&gt;

&lt;p&gt;What happens when we increase the number of observations to 100?&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;#nbr of obs
n &amp;lt;- 100
# Estimation
mc100 &amp;lt;- mcbeta(n,mc,k,tdf)
mc100 &amp;lt;- melt(mc100)

colnames(mc100) &amp;lt;- c('iteration','a','value','residuals')

gg.mc100 &amp;lt;- ggplot(mc100,aes(x=value,fill=residuals)) + geom_histogram(binwidth=.01, alpha=.5, position='identity')
print(gg.mc100)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-15-IE14-W3.rmd/mc100.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Both distributions are closer to the true value, but the &lt;em&gt;t&lt;/em&gt; distribution still produces many more outliers. Now with 1000 observations:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;#nbr of obs
n &amp;lt;- 1000
# Estimation
mc1000 &amp;lt;- mcbeta(n,mc,k,tdf)
mc1000 &amp;lt;- melt(mc1000)

colnames(mc1000) &amp;lt;- c('iteration','a','value','residuals')

gg.mc1000 &amp;lt;- ggplot(mc1000,aes(x=value,fill=residuals)) + geom_histogram(binwidth=.005, alpha=.5, position='identity')
print(gg.mc1000)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-15-IE14-W3.rmd/mc1000.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With 1000 observations the distribution the two distributions are similar, in this case the normal approximation is good but we still have more extreme observations with the &lt;em&gt;t&lt;/em&gt; innovations.&lt;/p&gt;

&lt;p&gt;Let’s keep 1000 observation but increase the number of degrees of freedoms of the &lt;em&gt;t&lt;/em&gt; distributions so that it generates fewer extreme innovations:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;#nbr of obs
tdf &amp;lt;- 10
# Estimation
mcdf10 &amp;lt;- mcbeta(n,mc,k,tdf)
mcdf10 &amp;lt;- melt(mcdf10)

colnames(mcdf10) &amp;lt;- c('iteration','a','value','residuals')

gg.mcdf10 &amp;lt;- ggplot(mcdf10,aes(x=value,fill=residuals)) + geom_histogram(binwidth=.005, alpha=.5, position='identity')
print(gg.mcdf10)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-15-IE14-W3.rmd/mcdf10.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With 10 degrees of freedom, the two distributions are almost overlapping, the normal approximation is a good one in this case.&lt;/p&gt;

&lt;h2 id=&quot;size-and-power-of-the-t-test&quot;&gt;Size and power of the t-test.&lt;/h2&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;#Let's create a function returning the mc estimates of beta 
# Inputs: - n the number of observations
#         - mc the number of monte carlo iterations
#         - k the number of control variables

# Output: 
mcsp &amp;lt;- function(n,mc){
  
  # vector of values b for which we test H0: \hat\beta = b
  b &amp;lt;- seq(0,1,0.01)
  #storage matrix
  tstats &amp;lt;- matrix(NA,nrow=mc,ncol=length(b))
  colnames(tstats) &amp;lt;- b
  # Looping over the number of iteratons 
  for(m in 1:mc){
    # We use a new set of controls for every experiment
    X &amp;lt;- matrix(rnorm(n),nrow=n,ncol=1)
    # standard normal innovations
    eps &amp;lt;- rnorm(n,sd=1)
    # the true beta is 1 
    y &amp;lt;- X + eps    
    # Storing the estimated beta with std.err
    smy &amp;lt;- summary(lm(y ~ X))$coefficients
    # Computing the vector of t-statistcs
    tstats[m,] &amp;lt;- (smy[2,1] - b)/smy[2,2]
    
  }
# returning the vector of t-statistics.
  return(tstats)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this first experiment we set n=100 and compute a 95\% t-test for the hypothesis \(\mathcal{H}_0:\hat\beta_i = b\) with b going from 0 to 1 (the true value) by steps of 0.01. We plot below the rejection frequency for each value of b.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;#nbr of obs
n &amp;lt;- 50
#nbr of Monte Carlo iterations
mc &amp;lt;- 1000
# Critical value for the t-test
cv &amp;lt;- 1.96
# running the experiments
sp100 &amp;lt;- mcsp(n,mc)

msp100 &amp;lt;- melt(colMeans(abs(sp100&amp;gt;cv)))
msp100 &amp;lt;- cbind(as.numeric(colnames(sp100)),msp100)
colnames(msp100) &amp;lt;- c('b','value')

gg.msp100 &amp;lt;- ggplot(msp100,aes(x=b,y=value)) + geom_line() 
print(gg.msp100)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-15-IE14-W3.rmd/ps100.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We now repeat the same experiment as before for varying sample size. Each ribbon in the plot shows the rejection frequency for every value of b (y axis) at a given sample size (x axis). So the size of the test is the rejection frequency at b=1, it is always close to 0.05 the nominal value, and the rejection frequencies for b&amp;gt;1 are estimates of the power of the test for a given sample sizes. The power always starts close to 0.05 and climbs towards 1 while we move away from the true hypothesis. The climb is faster as the sample size increases: the more information the more often we reject false hypotheses.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;#nbr of obs
b &amp;lt;- seq(0,1,0.01)
vobs &amp;lt;- c(5,10,15,20,25,30,35,40,45,50,60,70,80,90,100)
# Storage
sp &amp;lt;- sp2 &amp;lt;- matrix(NA,nrow=length(vobs),ncol=length(b))
#nbr of Monte Carlo iterations
mc &amp;lt;- 1000
# Critical value for the t-test
cv &amp;lt;- 1.96
cv2&amp;lt;- qt(0.975,df=vobs-2)
# Looping over the sample sizes. 
ncount &amp;lt;- 0
for(n in vobs){
	ncount &amp;lt;- ncount + 1
	# running the experiments
	sp100 &amp;lt;- mcsp(n,mc)
	sp[ncount,] &amp;lt;- colMeans(abs(sp100&amp;gt;cv))
	sp2[ncount,] &amp;lt;- colMeans(abs(sp100&amp;gt;cv2[ncount]))
	}
colnames(sp2) &amp;lt;- colnames(sp) &amp;lt;- b
rownames(sp2) &amp;lt;- rownames(sp) &amp;lt;- vobs


ribbon3D(x=vobs,y=b,z =sp, along ='y', clab = 'rejection frequency', shade = 0.5, bty = &amp;quot;g&amp;quot;, ticktype = &amp;quot;detailed&amp;quot;,phi=20,theta=-110)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-15-IE14-W3.rmd/sp.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that for the previous plot we used the quantile from the Gaussian distributions (i.e. from the asymptotic distribution of \(\mathcal{H}_0\)). In the plot below we use the exact Student critical values.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;ribbon3D(x=vobs,y=b,z =sp2, along ='y', clab = 'rejection frequency', shade = 0.5, bty = &amp;quot;g&amp;quot;, ticktype = &amp;quot;detailed&amp;quot;,phi=20,theta=-110)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-15-IE14-W3.rmd/sp2.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/ie15/IE14-W3&quot;&gt;Week 3:  Monte Carlos.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on February 15, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Week 2: R-squared experiment, t-test, F-test .]]></title>
  <link>/ie15/IE15-W2</link>
  <id>/ie15/IE15-W2</id>
  <published>2015-02-10T00:00:00+01:00</published>
  <updated>2015-02-10T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;h3 id=&quot;restricted-regression-with-the-wage-data&quot;&gt;Restricted regression with the wage data.&lt;/h3&gt;

&lt;p&gt;First we load the data and check that it looks as expected. Make sure to adjust the path to the location of the data on your computer.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Loading the data.
wage &amp;lt;- read.csv('../data/wages.csv')
# Printing the first rows of wage.
head(wage)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##   X EXPER MALE SCHOOL  WAGE
## 1 1     9    0     13 6.315
## 2 2    12    0     12 5.480
## 3 3    11    0     11 3.642
## 4 4     9    0     14 4.593
## 5 5     8    0     14 2.418
## 6 6     9    0     14 2.094&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we can regress &lt;em&gt;WAGE&lt;/em&gt; on &lt;em&gt;SCHOOL&lt;/em&gt; and &lt;em&gt;EXPER&lt;/em&gt; and &lt;em&gt;MALE&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# regression of wage on school and exper and male.
mod1 &amp;lt;- lm(WAGE ~ SCHOOL + EXPER + MALE, data=wage)
# printing the results of the regression.
summary(mod1)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
## Call:
## lm(formula = WAGE ~ SCHOOL + EXPER + MALE, data = wage)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -7.65  -1.97  -0.46   1.44  34.19 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  -3.3800     0.4650   -7.27  4.5e-13 ***
## SCHOOL        0.6388     0.0328   19.48  &amp;lt; 2e-16 ***
## EXPER         0.1248     0.0238    5.25  1.6e-07 ***
## MALE          1.3444     0.1077   12.49  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.05 on 3290 degrees of freedom
## Multiple R-squared:  0.133,	Adjusted R-squared:  0.132 
## F-statistic:  168 on 3 and 3290 DF,  p-value: &amp;lt;2e-16&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To compute a restricted regression with \(\beta_{SCHOOL} = \beta_{EXPER}\) we define a new variable &lt;em&gt;XPSC&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;wage$XPSC &amp;lt;- wage$EXPER - wage$SCHOOL
# regression of wage on xpsc and male.
mod2 &amp;lt;- lm(WAGE ~ XPSC + MALE, data=wage)
# printing the results of the regression.
summary(mod2)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
## Call:
## lm(formula = WAGE ~ XPSC + MALE, data = wage)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -6.60  -2.07  -0.48   1.52  33.05 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   4.5085     0.1099   41.02   &amp;lt;2e-16 ***
## XPSC         -0.1555     0.0183   -8.51   &amp;lt;2e-16 ***
## MALE          1.3199     0.1125   11.73   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.18 on 3291 degrees of freedom
## Multiple R-squared:  0.0526,	Adjusted R-squared:  0.052 
## F-statistic: 91.4 on 2 and 3291 DF,  p-value: &amp;lt;2e-16&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;#plotting library, install it if missing.
library(ggplot2)
library(reshape2)
# Number of observations
n &amp;lt;- 100
# Number of replications
mciter &amp;lt;- 10
# a vector with the number of control variables
k.all &amp;lt;- seq(from=0,to=n,by=round(n/50,0))
# storage initialization
r2 &amp;lt;- matrix(0,ncol=length(k.all),nrow=2)
colnames(r2) &amp;lt;- k.all
rownames(r2) &amp;lt;- c('R2','adjusted R2')
r2agg &amp;lt;- r2

for(j in 1:mciter){
	# y is just white noise
	y &amp;lt;-  matrix(rnorm(n),ncol=1)
	# a vector with the number of control variables
	k.all &amp;lt;- seq(from=0,to=n,by=round(n/50,0))
	# counter
	i &amp;lt;- 1
	# looping over the numbers of controls.
	for(k in k.all){
	    # creating the controls with a constant
	   X &amp;lt;- cbind(1,matrix(rnorm(n*(ifelse(k==0,0,k-1))),nrow=n,ncol=ifelse(k==0,0,k-1)))
	    # regression + computation of summary statistics
	    slm &amp;lt;- summary(lm(y ~ X))
	    # Storage of the resuts
	    r2[1,i] &amp;lt;- slm$r.squared
	    r2[2,i] &amp;lt;- slm$adj.r.squared
	    # increment the counter
	    i &amp;lt;- i+1
	}
	#Storing the result of the first xp only.
	if(j==1)mr2     &amp;lt;-  melt(r2)
	if(j==1)names(mr2)[1:2]  &amp;lt;-  c('r2','k')
	
	# aggregating the r2s
	 r2agg &amp;lt;- r2agg + r2

}
#formating the output
mr2agg    &amp;lt;-  melt(r2agg/mciter)
names(mr2agg)[1:2] &amp;lt;- c('r2','k')
options(warn = -1)
# creating the plots 
gr2 &amp;lt;- ggplot(mr2,aes(x=k,y=value,colour=r2)) + geom_line()
gr2agg &amp;lt;- ggplot(mr2agg,aes(x=k,y=value,colour=r2)) + geom_line()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Plot of the first iteration:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;print(gr2)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-10-IE15-W2.rmd/plotgr2.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Plot of the average over all iterations:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;print(gr2agg)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-10-IE15-W2.rmd/plotgr2agg.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/ie15/IE15-W2&quot;&gt;Week 2: R-squared experiment, t-test, F-test .&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on February 10, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Sharp threshold detection based on sup-norm error rates in high-dimensional models]]></title>
  <link>/papers/threshold</link>
  <id>/papers/threshold</id>
  <published>2015-02-09T00:00:00+01:00</published>
  <updated>2015-02-09T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;&lt;strong&gt;Submitted.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Download the paper &lt;a href=&quot;/papers/tlasso.pdf&quot;&gt;pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lcallot/ttlas&quot;&gt;Replication material&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract:&lt;/h2&gt;

&lt;p&gt;We propose a new estimator, the thresholded scaled Lasso, in high dimensional threshold regressions. First, we establish an upper bound on the \( \ell_{\infty} \) estimation error of the scaled Lasso estimator of Lee et al. (2012). This is a non-trivial task as the literature on high-dimensional models has focused almost exclusively on \( \ell_1 \) and \( \ell_2\) estimation errors. We show that this sup-norm bound can be used to distinguish between zero and non-zero coefficients at a much finer scale than would have been possible using classical oracle inequalities. Thus, our sup-norm bound is tailored to consistent variable selection via thresholding.&lt;/p&gt;

&lt;p&gt;Our simulations show that thresholding the scaled Lasso yields substantial improvements in terms of variable selection. Finally, we use our estimator to shed further empirical light on the long running debate on the relationship between the level of debt (public and private) and GDP growth.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/papers/threshold&quot;&gt;Sharp threshold detection based on sup-norm error rates in high-dimensional models&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on February 09, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Week 1: Multiple Regressions and Matrix Notation.]]></title>
  <link>/ie15/IE15-W1</link>
  <id>/ie15/IE15-W1</id>
  <published>2015-02-04T00:00:00+01:00</published>
  <updated>2015-02-04T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;h1 id=&quot;regressions-with-multiple-variables-and-matrix-notation&quot;&gt;Regressions with multiple variables and matrix notation&lt;/h1&gt;

&lt;p&gt;This week we discussed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Regressions with multiple variables.&lt;/li&gt;
  &lt;li&gt;The OLS in natrix notation.&lt;/li&gt;
  &lt;li&gt;Properties of the OLS  estimator.&lt;/li&gt;
  &lt;li&gt;Omitted variable bias.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post will focus on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generating artificial data.&lt;/li&gt;
  &lt;li&gt;Use matrices in the code.&lt;/li&gt;
  &lt;li&gt;Estimate the OLS using built in functions&lt;/li&gt;
  &lt;li&gt;Manually estimate the OLS.&lt;/li&gt;
  &lt;li&gt;Calculate the parameters standard errors.&lt;/li&gt;
  &lt;li&gt;Plot the density of the residuals.&lt;/li&gt;
  &lt;li&gt;Load the wage data and compute regressions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can get help on any function by typing &lt;em&gt;?FunctionName&lt;/em&gt; in the &lt;em&gt;R&lt;/em&gt; console, try &lt;em&gt;?dim&lt;/em&gt; for example.&lt;/p&gt;

&lt;h3 id=&quot;multiple-regression-with-the-wage-data&quot;&gt;Multiple regression with the wage data.&lt;/h3&gt;

&lt;p&gt;First we load the data and check that it looks as expected.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Loading the data.
wage &amp;lt;- read.csv('../data/wages.csv')
# Printing the first rows of wage.
head(wage)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##   X EXPER MALE SCHOOL  WAGE
## 1 1     9    0     13 6.315
## 2 2    12    0     12 5.480
## 3 3    11    0     11 3.642
## 4 4     9    0     14 4.593
## 5 5     8    0     14 2.418
## 6 6     9    0     14 2.094&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we can regress &lt;em&gt;WAGE&lt;/em&gt; on &lt;em&gt;SCHOOL&lt;/em&gt; and &lt;em&gt;EXPER&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# regression of wage on school and exper.
mod1 &amp;lt;- lm(WAGE ~ SCHOOL + EXPER, data=wage)
# printing the results of the regression.
summary(mod1)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
## Call:
## lm(formula = WAGE ~ SCHOOL + EXPER, data = wage)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -6.88  -1.99  -0.52   1.39  34.91 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  -2.4767     0.4700   -5.27  1.5e-07 ***
## SCHOOL        0.5992     0.0334   17.94  &amp;lt; 2e-16 ***
## EXPER         0.1573     0.0242    6.51  8.9e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.12 on 3291 degrees of freedom
## Multiple R-squared:  0.0915,	Adjusted R-squared:  0.0909 
## F-statistic:  166 on 2 and 3291 DF,  p-value: &amp;lt;2e-16&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;question&quot;&gt;Question:&lt;/h2&gt;
&lt;p&gt;Try estimating other models using different combination of variables.&lt;/p&gt;

&lt;h3 id=&quot;transformations-of-the-data&quot;&gt;Transformations of the data&lt;/h3&gt;

&lt;p&gt;Maybe the number of years at school does not enter linearily in the model. We can expand the model with a quadratic term:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Create a variable with the square of the number of school years.
wage$SCHOOL2 &amp;lt;- wage$SCHOOL^2
# Estimating the model and printing the results. 
mod2 &amp;lt;- lm(WAGE ~ SCHOOL + SCHOOL2 + EXPER, data=wage)
summary(mod2)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
## Call:
## lm(formula = WAGE ~ SCHOOL + SCHOOL2 + EXPER, data = wage)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -7.33  -1.96  -0.51   1.38  34.97 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   3.1149     1.5475    2.01  0.04421 *  
## SCHOOL       -0.3840     0.2614   -1.47  0.14201    
## SCHOOL2       0.0433     0.0114    3.79  0.00015 ***
## EXPER         0.1409     0.0245    5.75  9.8e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.11 on 3290 degrees of freedom
## Multiple R-squared:  0.0954,	Adjusted R-squared:  0.0946 
## F-statistic:  116 on 3 and 3290 DF,  p-value: &amp;lt;2e-16&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;question-1&quot;&gt;Question:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Interpret the results above.&lt;/li&gt;
  &lt;li&gt;Maybe the number of years of schooling has a different effect for males and for females. How would you investigate this?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;generating-data-following-a-regression-model-with-multiple-variables&quot;&gt;Generating data following a regression model with multiple variables&lt;/h3&gt;
&lt;p&gt;Let us assume the response variable &lt;em&gt;y&lt;/em&gt; follows a regression model with 3 explanatory variables:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
	y &amp; = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon \\
	y &amp; = (\iota,x_1,x_2,x_3)\beta + \epsilon \\
	y &amp; = X\beta + \epsilon
	\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where \( \epsilon_i \sim \mathcal{N}(0,1)\), \( \iota \) is a vector of ones (&lt;em&gt;i.e.&lt;/em&gt; the constant), and \( \beta = (1,0.5,1,-1)’\) is the \(4\times 1\) vector of parameters.&lt;/p&gt;

&lt;p&gt;We want to generate data following this regression model with &lt;em&gt;k=3&lt;/em&gt; regressors (plus the constant) and &lt;em&gt;n=100&lt;/em&gt; observations. We generate a matrix &lt;em&gt;X&lt;/em&gt; of dimensions \(n\times k\) filled with random normal values. For that we need the following &lt;em&gt;R&lt;/em&gt; functions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;matrix()&lt;/em&gt; to generate a matrix.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;rnrom()&lt;/em&gt; to generate \(\mathcal{N}(0,1)\) random numbers.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;dim()&lt;/em&gt; to print the dimensions of a matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Choosing the seed
set.seed(666)

# Setting up the number of variables and observations:
k &amp;lt;- 3
n &amp;lt;- 100

# Creating the matrix of regressors:
X &amp;lt;- matrix(rnorm(k*n),ncol=k,nrow=n)

# Creating the vector of parameters beta:
beta &amp;lt;- matrix(c(0.5,1,-1),ncol=1,nrow=3)

# printing the dimensions of X:
dim(X)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## [1] 100   3&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can now generate the variable &lt;em&gt;y&lt;/em&gt; using the matrix multiplication operator &lt;em&gt;%*%&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# First I generate the innovations:
eps &amp;lt;- rnorm(n)

# Then y:
y &amp;lt;- 1 + X%*%beta + eps

# And check that the dimensions make sense
dim(y)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## [1] 100   1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;estimating-beta-by-ols&quot;&gt;Estimating \(\beta\) by OLS&lt;/h3&gt;

&lt;p&gt;The generic function to estimate OLS in &lt;em&gt;R&lt;/em&gt; is called &lt;em&gt;lm()&lt;/em&gt;, notice that it automatically adds a constant to our model.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# estimate the model.
ols.lm &amp;lt;- lm(y ~ X)
# print the results. 
summary(ols.lm)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.905 -0.626 -0.110  0.675  2.171 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   0.9873     0.0987   10.00  &amp;lt; 2e-16 ***
## X1            0.4840     0.0972    4.98  2.8e-06 ***
## X2            1.2524     0.0881   14.21  &amp;lt; 2e-16 ***
## X3           -0.9078     0.0986   -9.20  7.6e-15 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.979 on 96 degrees of freedom
## Multiple R-squared:  0.78,	Adjusted R-squared:  0.773 
## F-statistic:  113 on 3 and 96 DF,  p-value: &amp;lt;2e-16&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The summary of the &lt;em&gt;lm()&lt;/em&gt; function shows many statistics that we will learn to understand throughout the course. If you try &lt;em&gt;plot(ols.lm)&lt;/em&gt; you will also get a number of useful plots, but… This is all too easy!&lt;/p&gt;

&lt;p&gt;It is more fun to code the OLS estimator ourselves. Remember the formula for the OLS estimator \(\hat{\beta}\) of \(\beta\) in matrix form: \(\hat{\beta}=\left(X’X\right)^{-1}X’y\). For that we need only two functions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The transpose function: &lt;em&gt;t()&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;The matrix inversion function &lt;em&gt;solve()&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also need to remember adding a constant to X using the &lt;em&gt;cbind()&lt;/em&gt; function.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Adding the constant
X &amp;lt;- cbind(1,X)

# Estimating beta hat
beta.hat &amp;lt;- solve(t(X)%*%X)%*%t(X)%*%y
print(beta.hat)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##         [,1]
## [1,]  0.9873
## [2,]  0.4840
## [3,]  1.2524
## [4,] -0.9078&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The estimates of \(\hat{\beta}\) are the same using both methods, that is nice.&lt;/p&gt;

&lt;h2 id=&quot;question-2&quot;&gt;Question:&lt;/h2&gt;
&lt;p&gt;Compute the standard errors of the residuals and of the OLS estimator. To do this, you will need to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Compute the OLS residuals:
    &lt;ul&gt;
      &lt;li&gt;compute the fit \( \hat{y}=X\hat{\beta} \),&lt;/li&gt;
      &lt;li&gt;compute the residuals \( \hat{\epsilon} = y - \hat{y} \). This should be a vector of the same length as &lt;em&gt;y&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Compute \(  \hat{\sigma}^2=\frac{\hat{\epsilon}’\hat{\epsilon}}{n-(k+1)}  \), to get the standard error you need to use the &lt;em&gt;sqrt()&lt;/em&gt; function on \(\hat{\sigma}^2\)&lt;/li&gt;
  &lt;li&gt;Get the individual standard errors:
    &lt;ul&gt;
      &lt;li&gt;Get the diagonal of \((X’X)^{-1}\) using &lt;em&gt;diag()&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Scale its square-root by \(\hat{\sigma}\). You should get a vector of standard errors of the same length as &lt;em&gt;beta.hat&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;To compare the output on your machine with the one in this post, make sure you have set the same seed as I have.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Computing the residuals
y.hat &amp;lt;- X%*%beta.hat
e.hat &amp;lt;- y-y.hat

# Residual standard error
sig2 &amp;lt;- (t(e.hat)%*%e.hat) / (n - (k+1))
res.se &amp;lt;- sqrt(sig2)
cat('\n residual standard errors:', res.se,sep='')&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
##  residual standard errors:0.9785&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Parameter std. err.
diag.xx &amp;lt;-diag(solve(t(X)%*%X))
beta.se &amp;lt;- res.se * sqrt(diag.xx)
cat('\n Parameters standard errors:', beta.se,sep='')&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
##  Parameters standard errors:0.098690.097170.088130.09863&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;visualizing-the-results&quot;&gt;Visualizing the results.&lt;/h3&gt;

&lt;p&gt;Let evaluate the quality of our model graphically. To do that we can look at a scatter plot of the observed values and the residuals.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Creating a data frame with the resituals, and the observed and fitted values.
ols.man &amp;lt;- data.frame(residuals = e.hat, fitted = y.hat, observed = y)
# Loading a plotting library.
require(ggplot2)
sqplot &amp;lt;- ggplot(ols.man, aes(y=observed,x=residuals)) + geom_point()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;print(sqplot)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-04-IE15-W1.rmd/fig-sq.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we can also look at the density of our residuals.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;densplot &amp;lt;- ggplot(ols.man, aes(x=residuals)) + geom_density()
print(densplot)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2015-02-04-IE15-W1.rmd/fig-dens.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;question-3&quot;&gt;Question:&lt;/h2&gt;

&lt;p&gt;Does this look like a Gaussian density to you? What happens when you re-run the code increasing the sample size &lt;em&gt;n&lt;/em&gt; to 1000?&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/ie15/IE15-W1&quot;&gt;Week 1: Multiple Regressions and Matrix Notation.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on February 04, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Introductory Econometrics (Inleiding Econometrie 1) 2015]]></title>
  <link>/teaching/IE15.html</link>
  <id>/teaching/IE2015_main</id>
  <updated>2015-02-04T00:00:00-00:00</updated>
  <published>2015-01-27T00:00:00+01:00</published>
  
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;I will use this page to publish code allowing you to replicate the empirical and simulation results presented during the classes. 
All the code in these posts is &lt;em&gt;R&lt;/em&gt; code. &lt;em&gt;R&lt;/em&gt; allows me to easily share code with you, which you can run and modify. The simplest way to do that is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install &lt;a href=&quot;http://www.rstudio.com&quot;&gt;Rstudio&lt;/a&gt; (free, open source, compatible with all platform),&lt;/li&gt;
  &lt;li&gt;Copy/paste the pieces of code below in Rstudio’s console,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;that’s it.&lt;/p&gt;

&lt;p&gt;These post also contains some computational questions. I encourage you to give these questions a try: if you really understand something, then you should be able to program it. Use whichever language/package you feel most comfortable with.&lt;/p&gt;

&lt;p&gt;I have enabled comments. I encourage you to use those to ask questions and reply to open questions if you know the answer. The comments are anonymous and not necessarily linked to your &lt;em&gt;vu&lt;/em&gt; email, please keep it clean.&lt;/p&gt;

&lt;article&gt;
&lt;h3&gt;&lt;a href=&quot;/ie15/IE15-W5&quot;&gt;Week 5:  Maximum Likelihood.&lt;/a&gt;&lt;/h3&gt;
&lt;!---&lt;span class=&quot;entry-date&quot;&gt;&lt;time datetime=&quot;2015-03-03T00:00:00+01:00&quot;&gt;March 2015&lt;/time&gt;&lt;/span&gt;--&gt;
&lt;/article&gt;

&lt;article&gt;
&lt;h3&gt;&lt;a href=&quot;/ie15/IE14-W3&quot;&gt;Week 3:  Monte Carlos.&lt;/a&gt;&lt;/h3&gt;
&lt;!---&lt;span class=&quot;entry-date&quot;&gt;&lt;time datetime=&quot;2015-02-15T00:00:00+01:00&quot;&gt;February 2015&lt;/time&gt;&lt;/span&gt;--&gt;
&lt;/article&gt;

&lt;article&gt;
&lt;h3&gt;&lt;a href=&quot;/ie15/IE15-W2&quot;&gt;Week 2: R-squared experiment, t-test, F-test .&lt;/a&gt;&lt;/h3&gt;
&lt;!---&lt;span class=&quot;entry-date&quot;&gt;&lt;time datetime=&quot;2015-02-10T00:00:00+01:00&quot;&gt;February 2015&lt;/time&gt;&lt;/span&gt;--&gt;
&lt;/article&gt;

&lt;article&gt;
&lt;h3&gt;&lt;a href=&quot;/ie15/IE15-W1&quot;&gt;Week 1: Multiple Regressions and Matrix Notation.&lt;/a&gt;&lt;/h3&gt;
&lt;!---&lt;span class=&quot;entry-date&quot;&gt;&lt;time datetime=&quot;2015-02-04T00:00:00+01:00&quot;&gt;February 2015&lt;/time&gt;&lt;/span&gt;--&gt;
&lt;/article&gt;


  &lt;p&gt;&lt;a href=&quot;/teaching/IE15.html&quot;&gt;Introductory Econometrics (Inleiding Econometrie 1) 2015&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on January 27, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Deterministic and stochastic trends in the Lee-Carter mortality model.]]></title>
  <link>/papers/death</link>
  <id>/papers/death</id>
  <published>2014-11-20T00:00:00+01:00</published>
  <updated>2014-11-20T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;&lt;strong&gt;Submitted.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Download the paper &lt;a href=&quot;/papers/death_wp.pdf&quot;&gt;pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lcallot/LeeCarter&quot;&gt;Replication material&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract:&lt;/h2&gt;

&lt;p&gt;The Lee and Carter (1992) model assumes that the deterministic and stochastic time series dynamics loads with identical weights when describing the development of age specific mortality rates. Effectively this means that the main characteristics of the model simplifies to a random walk model with age specific drift components. But restricting the adjustment mechanism of the stochastic and linear trend components to be identical may be a too strong simplification. In fact, the presence of a stochastic trend component may itself result from a bias induced by properly fitting the linear trend that characterizes mortality data. We find empirical evidence that this feature
of the Lee-Carter model overly restricts the system dynamics and we suggest to separate the deterministic and stochastic time series components at the benefit of improved fit and forecasting performance. In fact, we find that the classical Lee-Carter model will otherwise over estimate the reduction of mortality for the younger age groups and will under estimate the reduction of mortality for
the older age groups. In practice, our recommendation means that the Lee-Carter model instead of a one-factor model should be formulated as a two (or several)-factor model where one factor is deterministic and the other factors are stochastic. This feature generalizes to the range of models that extend the Lee-Carter model in various directions.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/papers/death&quot;&gt;Deterministic and stochastic trends in the Lee-Carter mortality model.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on November 20, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Estimation and Forecasting of Large Realized Covariance Matrices and Portfolio Choice.]]></title>
  <link>/papers/rcv-fc</link>
  <id>/papers/rcv-fc</id>
  <published>2014-11-13T00:00:00+01:00</published>
  <updated>2014-11-13T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;&lt;strong&gt;Submitted.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Download the paper &lt;a href=&quot;/papers/rcv_forecasts.pdf&quot;&gt;pdf&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract:&lt;/h2&gt;

&lt;p&gt;In this paper we consider modeling and forecasting of large realized covariance matrices by penalized vector autoregressive models. We propose using Lasso-type estimators to reduce the dimensionality to a manageable one and provide strong theoretical performance guarantees on the forecast capability of our procedure. To be precise, we show that we can forecast future realized covariance matrices almost as precisely as if we had known the true driving dynamics of these in advance. We next investigate the sources of these driving dynamics for the realized covariance matrices of the 30 Dow Jones stocks and find that these dynamics are not stable as the data is aggregated from the daily to the weekly and monthly frequency.&lt;/p&gt;

&lt;p&gt;The theoretical performance guarantees on our forecasts are illustrated on the Dow Jones index. In particular, we can beat our benchmark by a wide margin at the longer forecast horizons. Finally, we investigate the economic value of our forecasts in a portfolio selection exercise and find that in certain cases an investor is willing to pay a considerable amount in order get access to our forecasts.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/papers/rcv-fc&quot;&gt;Estimation and Forecasting of Large Realized Covariance Matrices and Portfolio Choice.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on November 13, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Vector Autoregressions with Parsimoniously Time Varying Parameters and an Application to Monetary Policy.]]></title>
  <link>/papers/ptv-var</link>
  <id>/papers/ptv-var</id>
  <published>2014-11-04T00:00:00+01:00</published>
  <updated>2014-11-04T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;&lt;strong&gt;Submitted.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Download the paper &lt;a href=&quot;/papers/ptv-var_WP.pdf&quot;&gt;pdf&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract:&lt;/h2&gt;

&lt;p&gt;This paper proposes a parsimoniously time varying parameter vector autoregressive model (with exogenous variables, VARX) and studies the properties of the Lasso and adaptive Lasso as estimators of this model. The parameters of the model are assumed to follow parsimonious random walks, where parsimony stems from the assumption that increments to the
parameters have a non-zero probability of being exactly equal to zero. By varying the degree of parsimony our model can accommodate constant parameters, an unknown number of
structural breaks, or parameters with a high degree of variation. We characterize the finite sample properties of the Lasso by deriving upper bounds on the estimation and prediction errors that are valid with high probability; and asymptotically we show that these bounds tend to zero with probability tending to one if the number of non zero increments grows slower than \( \sqrt{T} \).&lt;/p&gt;

&lt;p&gt;By simulation experiments we investigate the properties of the Lasso and the adaptive Lasso in settings where the parameters are stable, experience structural breaks, or follow a parsimonious random walk. We use our model to investigate the monetary policy response to inflation and business cycle fluctuations in the US by estimating a parsimoniously time varying parameter Taylor rule. We document substantial changes in the policy response of the Fed in the 1980s and since 2008.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/papers/ptv-var&quot;&gt;Vector Autoregressions with Parsimoniously Time Varying Parameters and an Application to Monetary Policy.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on November 04, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Week 6: Empirical size and power of Likelihood based tests.]]></title>
  <link>/ie/IE14-W6</link>
  <id>/ie/IE14-W6</id>
  <published>2014-03-09T00:00:00+01:00</published>
  <updated>2014-03-09T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;h1 id=&quot;regressions-with-multiple-variables-and-matrix-notation&quot;&gt;Regressions with multiple variables and matrix notation&lt;/h1&gt;

&lt;h2 id=&quot;monte-carlo-estimation-of-the-size-and-power-of-likelihood-based-tests&quot;&gt;Monte Carlo estimation of the size and power of Likelihood based tests.&lt;/h2&gt;

&lt;p&gt;First, we create a function that takes as input a matrix X, the true value of beta2 as well as the number of observations and the number of Monte Carlo iterations. It returns a vector with the average rejection frequency of \(H0: \beta_2=0 \).&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;library(&amp;quot;ggplot2&amp;quot;)
# Create a function returning mc p-values for each test Inputs: - n the
# number of observations - mc the number of monte carlo iterations - b the
# true value of beta2 - X the X matrix Output:
mcMLtests &amp;lt;- function(n, mc, b, X) {
    
    # The parameter vector
    beta &amp;lt;- c(1, b)
    
    # storage matrix
    LR &amp;lt;- matrix(NA, nrow = mc, ncol = 1)
    LM &amp;lt;- matrix(NA, nrow = mc, ncol = 1)
    W &amp;lt;- matrix(NA, nrow = mc, ncol = 1)
    
    for (m in 1:mc) {
        eps &amp;lt;- rnorm(n, sd = 1)  # standard normal innovations
        
        # Generating the data
        y &amp;lt;- X %*% beta + eps
        
        # Estimating the model under the alternative
        ML1 &amp;lt;- lm(y ~ X)
        # Under the Null
        ML0 &amp;lt;- lm(y ~ X[, 1])
        
        # Storing the residuals
        eps0 &amp;lt;- resid(ML0)
        eps1 &amp;lt;- resid(ML1)
        
        # Storing the estimated betas
        beta0 &amp;lt;- coef(ML0)
        beta1 &amp;lt;- coef(ML1)
        
        # Calculating the MLE of sigma2
        sig0 &amp;lt;- sum(eps0^2)/n
        sig1 &amp;lt;- sum(eps1^2)/n
        
        # Computing the test statistics:
        LR[m] &amp;lt;- n * (log(sig0) - log(sig1))
        W[m] &amp;lt;- (beta1[3]^2)/(sqrt(sig1) * (1/sum(X[, 2]^2)))
        LM[m] &amp;lt;- (t(eps0) %*% X %*% solve(t(X) %*% X) %*% t(X) %*% eps0)/sig0
    }
    
    pval &amp;lt;- c(mean((1 - pchisq(LM, df = 1)) &amp;lt; 0.05), mean((1 - pchisq(LR, df = 1)) &amp;lt; 
        0.05), mean((1 - pchisq(W, df = 1)) &amp;lt; 0.05))
    return(pval)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We use the function created above to run a first experiment with 10 observations (n=10). The true value of beta2 varies in the range \(\beta_2 = (0,0.05,0.1,…,0.95,1)\).&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# nbr of obs
n &amp;lt;- 10
# nbr of Monte Carlo iterations
mc &amp;lt;- 1000
# beta2 parameter value
beta2 &amp;lt;- seq(from = 0, to = 1, by = 0.05)

# The storage matrix
mc10 &amp;lt;- matrix(NA, nrow = length(beta2), ncol = 3)
rownames(mc10) &amp;lt;- beta2
colnames(mc10) &amp;lt;- c(&amp;quot;LM&amp;quot;, &amp;quot;LR&amp;quot;, &amp;quot;W&amp;quot;)

# Same set of controls for every experiment
X &amp;lt;- matrix(rnorm(n * 2), nrow = n, ncol = 2)

# loop counter
i &amp;lt;- 1

# Monte Carlo experiment
for (b in beta2) {
    mc10[i, ] &amp;lt;- mcMLtests(n, mc, b, X)
    i &amp;lt;- i + 1
}

# Now preparing a plot of the results:
mc10 &amp;lt;- melt(mc10)
head(mc10)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##   Var1 Var2 value
## 1 0.00   LM 0.059
## 2 0.05   LM 0.083
## 3 0.10   LM 0.083
## 4 0.15   LM 0.089
## 5 0.20   LM 0.115
## 6 0.25   LM 0.132&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;colnames(mc10) &amp;lt;- c(&amp;quot;b&amp;quot;, &amp;quot;Test&amp;quot;, &amp;quot;Power&amp;quot;)

gg.mc10 &amp;lt;- ggplot(mc10, aes(x = b, y = Power, colour = Test)) + geom_line() + 
    ggtitle(&amp;quot;Rejection frequency of H0: beta2=0&amp;quot;)
print(gg.mc10)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2014-03-09-IE14-W6_rmd/mc10.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Another experiment with 100 observations and \( \beta_2 =0,0.025,0.05,…,0.475,0.5 \):&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# nbr of obs
n &amp;lt;- 100
# nbr of Monte Carlo iterations
mc &amp;lt;- 1000
# beta2 parameter value
beta2 &amp;lt;- seq(from = 0, to = 0.5, by = 0.025)

# The storage matrix
mc100 &amp;lt;- matrix(NA, nrow = length(beta2), ncol = 3)
rownames(mc100) &amp;lt;- beta2
colnames(mc100) &amp;lt;- c(&amp;quot;LM&amp;quot;, &amp;quot;LR&amp;quot;, &amp;quot;W&amp;quot;)

# Same set of controls for every experiment
X &amp;lt;- matrix(rnorm(n * 2), nrow = n, ncol = 2)

# loop counter
i &amp;lt;- 1

# Monte Carlo experiment
for (b in beta2) {
    mc100[i, ] &amp;lt;- mcMLtests(n, mc, b, X)
    i &amp;lt;- i + 1
}

# Now preparing a plot of the results:
mc100 &amp;lt;- melt(mc100)
head(mc100)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##    Var1 Var2 value
## 1 0.000   LM 0.056
## 2 0.025   LM 0.068
## 3 0.050   LM 0.076
## 4 0.075   LM 0.116
## 5 0.100   LM 0.147
## 6 0.125   LM 0.198&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;colnames(mc100) &amp;lt;- c(&amp;quot;b&amp;quot;, &amp;quot;Test&amp;quot;, &amp;quot;Power&amp;quot;)

gg.mc100 &amp;lt;- ggplot(mc100, aes(x = b, y = Power, colour = Test)) + geom_line() + 
    ggtitle(&amp;quot;Rejection frequency of H0: beta2=0&amp;quot;)
print(gg.mc100)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2014-03-09-IE14-W6_rmd/mc100.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Same experiment as before, but this time X is t-distributed with 3 degrees of freedom.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# The storage matrix
mc100X &amp;lt;- matrix(NA, nrow = length(beta2), ncol = 3)
rownames(mc100X) &amp;lt;- beta2
colnames(mc100X) &amp;lt;- c(&amp;quot;LM&amp;quot;, &amp;quot;LR&amp;quot;, &amp;quot;W&amp;quot;)

# Same set of controls for every experiment
X &amp;lt;- matrix(rt(n * 2, df = 3), ncol = 2, nrow = n)

# loop counter
i &amp;lt;- 1

# Monte Carlo experiment
for (b in beta2) {
    mc100X[i, ] &amp;lt;- mcMLtests(n, mc, b, X)
    i &amp;lt;- i + 1
}

# Now preparing a plot of the results:
mc100X &amp;lt;- melt(mc100X)
head(mc100X)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##    Var1 Var2 value
## 1 0.000   LM 0.053
## 2 0.025   LM 0.070
## 3 0.050   LM 0.173
## 4 0.075   LM 0.317
## 5 0.100   LM 0.522
## 6 0.125   LM 0.686&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;colnames(mc100X) &amp;lt;- c(&amp;quot;b&amp;quot;, &amp;quot;Test&amp;quot;, &amp;quot;Power&amp;quot;)

gg.mc100X &amp;lt;- ggplot(mc100X, aes(x = b, y = Power, colour = Test)) + geom_line() + 
    ggtitle(&amp;quot;Rejection frequency of H0: beta2=0&amp;quot;)
print(gg.mc100X)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2014-03-09-IE14-W6_rmd/mc100X.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/ie/IE14-W6&quot;&gt;Week 6: Empirical size and power of Likelihood based tests.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on March 09, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Software on rdam and arhus.]]></title>
  <link>/computing/software-rdam</link>
  <id>/computing/software-rdam</id>
  <published>2014-03-04T00:00:00+01:00</published>
  <updated>2014-03-04T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;This page describes some useful Linux tools and statistical programs available on &lt;em&gt;rdam&lt;/em&gt; (and on &lt;em&gt;arhus&lt;/em&gt; in most instances). These are very short introductions, internet is full of more thorough discussion of each of the topics presented here.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;rdam&lt;/em&gt; and &lt;em&gt;arhus&lt;/em&gt; have no scheduler and no limits are imposed on the size and the duration of the jobs that users may submit. &lt;strong&gt;With great power comes great responsibility:&lt;/strong&gt; you are responsible for ensuring that your jobs do not overload the servers. Below I describe a couple of tools useful for doing that.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;screen&lt;/strong&gt;: Creates and manages virtual sessions that survive after you logout and can be resumed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;top&lt;/strong&gt;: Monitor the server and user processes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main statistical packages available on &lt;em&gt;rdam&lt;/em&gt; are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;R&lt;/strong&gt;, &lt;strong&gt;Rstudio&lt;/strong&gt;, and &lt;strong&gt;Rstudio server&lt;/strong&gt;: Standard installation of &lt;em&gt;R&lt;/em&gt; and &lt;em&gt;Rstudio&lt;/em&gt; as well as a browser version of &lt;em&gt;Rstudio&lt;/em&gt; reachable through a SSH tunnel.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Matlab R2013b&lt;/strong&gt;: Standard installation with VU license.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ox 7.00&lt;/strong&gt;: Installed so that &lt;em&gt;Ox&lt;/em&gt; runs by default on a single thread.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;linux-tools&quot;&gt;Linux tools&lt;/h1&gt;

&lt;h4 id=&quot;screen&quot;&gt;screen&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;To create a virtual session named &lt;em&gt;sname&lt;/em&gt; using screen enter &lt;code&gt;screen -S sname&lt;/code&gt;. This will result in a blank command prompt, you are in the virtual session. Any job you launch in this session will continue even after you log out.&lt;/li&gt;
  &lt;li&gt;To get back to the main session (detach the screen) type &lt;code&gt;Ctrl-A&lt;/code&gt; and then &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;To reattach a detached session by entering &lt;code&gt;screen -r sname&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;To kill the &lt;em&gt;sname&lt;/em&gt; session enter &lt;code&gt;exit&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;To list the existing screen sessions with their status &lt;code&gt;screen -ls&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can find much more detailed (and fancy) tutorials and tips &lt;a href=&quot;http://www.tecmint.com/screen-command-examples-to-manage-linux-terminals/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://www.rackaid.com/blog/linux-screen-tutorial-and-how-to/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;top&quot;&gt;top&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;top&lt;/em&gt; is a useful monitoring tool. to access it, simply type &lt;code&gt;top&lt;/code&gt; at the command line. The first few lines that appear contain server wide statistics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The third line shows the share of total cpu used for:
    &lt;ol&gt;
      &lt;li&gt;user processes&lt;/li&gt;
      &lt;li&gt;system&lt;/li&gt;
      &lt;li&gt;nice (low priority) processes&lt;/li&gt;
      &lt;li&gt;idle&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;The fourth line shows the total, used, and available memory.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below comes a list of every process, starting with the process id and the user it belongs as well as memory use (RES) and cpu used (%CPU).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Be extremely careful that the memory is not overloaded lest the server grinds to a halt.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;rdam&lt;/em&gt; has 16 physical cores and can run on up to 32 cores thanks to hyper-threading however performances are optimal when only 16 cores are used. Do not start large jobs when the server is already loaded, you have &lt;a href=&quot;/computing/computing-resources-VU&quot;&gt;other machines&lt;/a&gt; at your disposal.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;software&quot;&gt;Software&lt;/h1&gt;

&lt;h4 id=&quot;r-rstudio-and-rstudio-server&quot;&gt;R, Rstudio, and Rstudio server&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;R&lt;/em&gt; is started by simply typing &lt;code&gt;R&lt;/code&gt; at the command line.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.rstudio.com/&quot;&gt;&lt;em&gt;Rstudio&lt;/em&gt;&lt;/a&gt; provides a graphical user interface for &lt;em&gt;R&lt;/em&gt; as well as seamless integration with &lt;a href=&quot;http://yihui.name/knitr/&quot;&gt;knitr&lt;/a&gt; (great tool for reproducible research) and Git (and &lt;a href=&quot;https://github.com/&quot;&gt;Github&lt;/a&gt;, the host of this site) for version control (useful to keep track of code changes and for collaborative work)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Rstudio Server&lt;/em&gt; is also installed and &lt;em&gt;rdam&lt;/em&gt;. It provides an access to &lt;em&gt;Rstudio&lt;/em&gt; on &lt;em&gt;rdam&lt;/em&gt; in your browser through a SSH tunnel in two steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Open a terminal on your machine and enter: &lt;code&gt;ssh -f -N -L 1234:localhost:8787 VUNetID@rdam&lt;/code&gt;. After you enter your password nothing happens, the SSH tunnel is in the background. How to do this using PuTTY is described &lt;a href=&quot;http://jimhester.com/post/rstudio-server-through-ssh-tunnel&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Open a browser and point it to &lt;code&gt;localhost:1234&lt;/code&gt;, enter your VUNetID and password, and here you have &lt;em&gt;Rstudio&lt;/em&gt; running on &lt;em&gt;rdam&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;matlab&quot;&gt;Matlab&lt;/h4&gt;

&lt;p&gt;Matlab is installed with the latest stable release R2013b.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start the GUI by typing &lt;code&gt;matlab&lt;/code&gt; at the command prompt.&lt;/li&gt;
  &lt;li&gt;To execute matlab scripts in the backgroup type &lt;code&gt;m myfile.m&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;ox-700&quot;&gt;Ox 7.00&lt;/h4&gt;

&lt;p&gt;Ox 7.00 is installed, usage is described on the website of &lt;a href=&quot;http://personal.vu.nl/c.s.bos/resources.html&quot;&gt;Charles Bos&lt;/a&gt;.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/computing/software-rdam&quot;&gt;Software on rdam and arhus.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on March 04, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Computing resources at Econometrics and OR.]]></title>
  <link>/computing/computing-resources-VU</link>
  <id>/computing/computing-resources-VU</id>
  <published>2014-02-28T00:00:00+01:00</published>
  <updated>2014-02-28T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Here is a brief overview of the machines readily accessible to members of the Econometrics &amp;amp; OR department. Modalities of access and usage of these machines is discussed below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Machine&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Cores&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Memory&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Remarks&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;em&gt;rdam&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;16 @2.9GHz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;128GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Detailed user guide below.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;em&gt;arhus&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8 @2.8GHz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12 GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Same as above.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;em&gt;pcoms001&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12 @2.3GHz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;128 GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Administered by the VU.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;em&gt;pcoms002&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12 @2.3GHz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;128 GB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Same as above.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;em&gt;Lisa&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6528 @2.2GHz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;16TB&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Administered by Surf Sara.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;pcoms0001-and-002&quot;&gt;pcoms0001 and 002&lt;/h1&gt;

&lt;p&gt;These machines are administered by the &lt;a href=&quot;https://vunet.login.vu.nl/services/pages/detail.aspx?cid=tcm%3a164-330191-16&quot;&gt;VU compute service&lt;/a&gt; and accessible from within the VU or through &lt;em&gt;rdam&lt;/em&gt; and &lt;em&gt;arhus&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A user manual is provided here &lt;a href=&quot;https://vunet.login.vu.nl/_layouts/SharePoint.Tridion.WebParts/redirect.aspx?cid=tcm:164-327430-16&quot;&gt;(pdf)&lt;/a&gt;. These are essentially similar in software and specs to &lt;em&gt;rdam&lt;/em&gt; so also refer to the guide below. &lt;em&gt;Note that jobs lasting over 24 hours run the risk of being killed without sommations&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;lisa&quot;&gt;Lisa&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://www.surfsara.nl/systems/lisa/description&quot;&gt;Lisa&lt;/a&gt; is a cluster administered by &lt;a href=&quot;https://www.surfsara.nl&quot;&gt;Surf Sara&lt;/a&gt; (who also provide access to even larger systems) composed of about 500 nodes, each with 8 to 16 cores and 24GB to 32GB of memory. Usage is well &lt;a href=&quot;https://www.surfsara.nl/systems/lisa/usage&quot;&gt;documented here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Access to &lt;em&gt;Lisa&lt;/em&gt; is subject to a simple and fast &lt;a href=&quot;https://www.surfsara.nl/systems/lisa/account&quot;&gt;application process&lt;/a&gt; which needs to be supported (by email) by a faculty member with a permanent position.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;rotterdam-and-arhus&quot;&gt;Rotterdam and Arhus&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Rotterdam&lt;/em&gt;, or &lt;em&gt;rdam&lt;/em&gt;, and &lt;em&gt;arhus&lt;/em&gt; are the names of the server financed by the Department of Econometrics &amp;amp; O.R., or NWO, at the VU University
Amsterdam. Members of the department can get an account from &lt;a href=&quot;mailto:l.callot@vu.nl&quot;&gt;Laurent Callot&lt;/a&gt; or &lt;a href=&quot;mailto:c.s.bos@vu.nl&quot;&gt;Charles
Bos&lt;/a&gt; who also maintain these machines.&lt;/p&gt;

&lt;h4 id=&quot;some-definitions&quot;&gt;Some definitions&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;rdam&lt;/em&gt; and &lt;em&gt;arhus&lt;/em&gt; are the machine for which you got an account.&lt;/li&gt;
  &lt;li&gt;They are known by IP address &lt;code&gt;145.108.238.26&lt;/code&gt; for rdam and &lt;code&gt;145.108.238.16&lt;/code&gt; for arhus. They are reachable
from outside the university as well, at these addresses.&lt;/li&gt;
  &lt;li&gt;You received your own login and password. Your login is personal, you
cannot give your password to anybody else.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;rdam&lt;/em&gt; contains a Tesla card, for GPU computations.&lt;/li&gt;
  &lt;li&gt;Each user has a limited amount of priority, e.g. if two users are working at the machine usually each is limited to 50% of the CPU-time if there is a conflict.&lt;/li&gt;
  &lt;li&gt;For the memory, there is no automatic limit. Please make sure you do not use too much memory, else the machine becomes very slow.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;entering-the-machine&quot;&gt;Entering the machine&lt;/h4&gt;

&lt;p&gt;If this is the first time you login, you will be asked to change your password.&lt;/p&gt;

&lt;h5 id=&quot;linux-unix-apple-stuff&quot;&gt;Linux, Unix, Apple stuff&lt;/h5&gt;
&lt;p&gt;To log onto &lt;em&gt;rdam&lt;/em&gt; start a terminal and enter &lt;code&gt;ssh VUnetID@145.108.238.26&lt;/code&gt;, you will be prompted for you password. To exit simply type &lt;code&gt;exit&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&quot;windows-machine&quot;&gt;Windows machine&lt;/h5&gt;
&lt;p&gt;You can enter the machine using a secure shell program, e.g. &lt;a href=&quot;http://www.chiark.greenend.org.uk/%7Esgtatham/putty/&quot;&gt;PuTTy&lt;/a&gt;. At the VU, send an email to &lt;a href=&quot;mailto:ucit-servicedesk@vu.nl&quot;&gt;ucit-servicedesk@vu.nl&lt;/a&gt; stating that you want access to PuTTY (if you don’t have it already).&lt;/p&gt;

&lt;p&gt;Start PuTTY, fill in as host name &lt;code&gt;145.108.238.26&lt;/code&gt; and save the session under the name &lt;em&gt;rdam&lt;/em&gt;, then hit the Open button. Fill in your login and password.  When you finish with PuTTY, type &lt;code&gt;exit&lt;/code&gt;, &lt;code&gt;ctrl-d&lt;/code&gt;, or just kill the window.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/computing/computing-resources-VU&quot;&gt;Computing resources at Econometrics and OR.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on February 28, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Week 2-3: R squared, Testing, and Monte Carlo.]]></title>
  <link>/ie/IE14-W23</link>
  <id>/ie/IE14-W23</id>
  <published>2014-02-17T00:00:00+01:00</published>
  <updated>2014-02-17T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;h1 id=&quot;regressions-with-multiple-variables-and-matrix-notation&quot;&gt;Regressions with multiple variables and matrix notation&lt;/h1&gt;

&lt;p&gt;These last 2 weeks we discussed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The R squared.&lt;/li&gt;
  &lt;li&gt;Test for linear restrictions.&lt;/li&gt;
  &lt;li&gt;Asymptotic properties of the OLS  estimator.&lt;/li&gt;
  &lt;li&gt;Omitted variable bias.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post will focus on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Estimate empirical distributions of the OLS estimator.&lt;/li&gt;
  &lt;li&gt;Investigate the finite sample size and power of tests.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;r-squared-with-increasing-number-of-regressors&quot;&gt;R Squared with increasing number of regressors&lt;/h3&gt;

&lt;p&gt;Below the code used to generate the R squared plots in the homework slides of Week 2&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# loading the plotting library
library(&amp;quot;ggplot2&amp;quot;)

# setting the seed
set.seed(42)

# Number of observations
n &amp;lt;- 100

# y is just white noise
y &amp;lt;- matrix(rnorm(n), ncol = 1)

# a vector with the number of control variables
k.all &amp;lt;- seq(from = 0, to = n, by = round(n/50, 0))

# storage initialization
r2 &amp;lt;- matrix(NA, ncol = length(k.all), nrow = 2)
colnames(r2) &amp;lt;- k.all
rownames(r2) &amp;lt;- c(&amp;quot;R2&amp;quot;, &amp;quot;adjusted R2&amp;quot;)

# counter
i &amp;lt;- 1
# looping over the numbers of controls.
for (k in k.all) {
    # creating the controls with a constant
    
    X &amp;lt;- cbind(1, matrix(rnorm(n * (ifelse(k == 0, 0, k - 1))), nrow = n, ncol = ifelse(k == 
        0, 0, k - 1)))
    
    # regression + computation of summary statistics
    slm &amp;lt;- summary(lm(y ~ X))
    
    # Storage of the resuts
    r2[1, i] &amp;lt;- slm$r.squared
    r2[2, i] &amp;lt;- slm$adj.r.squared
    
    # increment the counter
    i &amp;lt;- i + 1
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# plotting the results:

# preparing the data
mr2 &amp;lt;- melt(r2)
names(mr2)[1:2] &amp;lt;- c(&amp;quot;mesure&amp;quot;, &amp;quot;k&amp;quot;)

# plotting
gr2 &amp;lt;- ggplot(mr2, aes(x = k, y = value, colour = mesure)) + geom_line()
print(gr2)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2014-02-17-IE14-W23_rmd/plotr2.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;finite-sample-distribution-of-the-ols-estimator&quot;&gt;Finite sample distribution of the OLS estimator&lt;/h2&gt;

&lt;p&gt;\( \hat{\beta} \) is normaly distruted if the residuals are Gaussian, but it is asymptotically normal under weaker assumptions. How good is the gaussian approximation? We evaluate this using a Monte Carlo experiment.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Let's create a function returning the mc estimates of beta Inputs: - n the
# number of observations - mc the number of monte carlo iterations - k the
# number of control variables - tdf the degrees of freedom of the t
# distribution Output:
mcbeta &amp;lt;- function(n, mc, k, tdf) {
    
    # We use the same set of controls for every experiment
    X &amp;lt;- matrix(rnorm(n * k), nrow = n, ncol = k)
    
    # storage matrix
    bN &amp;lt;- matrix(NA, nrow = mc, ncol = k)
    bT &amp;lt;- matrix(NA, nrow = mc, ncol = k)
    
    for (m in 1:mc) {
        epsN &amp;lt;- rnorm(n, sd = sqrt(tdf/(tdf - 2)))  # standard normal innovations
        epsT &amp;lt;- rt(n, tdf)  #t(1) innovations
        
        # the true beta is 1 in both cases
        yN &amp;lt;- X + epsN
        yT &amp;lt;- X + epsT
        
        # Storing the estimated beta, discarding the constant
        bN[m, ] &amp;lt;- coef(lm(yN ~ X))[-1]
        bT[m, ] &amp;lt;- coef(lm(yT ~ X))[-1]
        
        # returning a list with the results
    }
    return(list(Normal = bN, Student = bT))
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now let’s run a first experiment with a very small sample of n=10 and plot the results as two overlapping histograms:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# nbr of obs
n &amp;lt;- 10
# nbr of Monte Carlo iterations
mc &amp;lt;- 1000
# nbr of controls
k &amp;lt;- 1
# degrees of freedom
tdf &amp;lt;- 3

mc10 &amp;lt;- mcbeta(n, mc, k, tdf)
mc10 &amp;lt;- melt(mc10)

colnames(mc10) &amp;lt;- c(&amp;quot;iteration&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;value&amp;quot;, &amp;quot;residuals&amp;quot;)

gg.mc10 &amp;lt;- ggplot(mc10, aes(x = value, fill = residuals)) + geom_histogram(binwidth = 0.1, 
    alpha = 0.5, position = &amp;quot;identity&amp;quot;)
print(gg.mc10)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2014-02-17-IE14-W23_rmd/mc10.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;\(\hat{\beta} \) is estimated to be somewhere between -1 and 4 when the innovations are &lt;em&gt;t(3)&lt;/em&gt;, while when they are &lt;em&gt;N(0,3)&lt;/em&gt; the range is around [0,2.5]. The &lt;em&gt;t&lt;/em&gt; distributed innovations produce many extreme estimates of \( \beta \). Because we seek to minimize the squared residuals, OLS is very sensitive to extreme observations.&lt;/p&gt;

&lt;p&gt;What happens when we increase the number of observations to 100?&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# nbr of obs
n &amp;lt;- 100
mc100 &amp;lt;- mcbeta(n, mc, k, tdf)
mc100 &amp;lt;- melt(mc100)

colnames(mc100) &amp;lt;- c(&amp;quot;iteration&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;value&amp;quot;, &amp;quot;residuals&amp;quot;)

gg.mc100 &amp;lt;- ggplot(mc100, aes(x = value, fill = residuals)) + geom_histogram(binwidth = 0.01, 
    alpha = 0.5, position = &amp;quot;identity&amp;quot;)
print(gg.mc100)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2014-02-17-IE14-W23_rmd/mc100.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Both distributions are closer to the true value, but the &lt;em&gt;t&lt;/em&gt; distribution still produces many more outliers. Now with 1000 observations:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# nbr of obs
n &amp;lt;- 1000
mc1000 &amp;lt;- mcbeta(n, mc, k, tdf)
mc1000 &amp;lt;- melt(mc1000)

colnames(mc1000) &amp;lt;- c(&amp;quot;iteration&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;value&amp;quot;, &amp;quot;residuals&amp;quot;)

gg.mc1000 &amp;lt;- ggplot(mc1000, aes(x = value, fill = residuals)) + geom_histogram(binwidth = 0.005, 
    alpha = 0.5, position = &amp;quot;identity&amp;quot;)
print(gg.mc1000)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2014-02-17-IE14-W23_rmd/mc1000.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With 1000 observations the distribution the two distributions are similar, in this case the normal approximation is good but we still have more extreme observations with the &lt;em&gt;t&lt;/em&gt; innovations.&lt;/p&gt;

&lt;p&gt;Let’s keep 1000 observation but increase the number of degrees of freedoms of the &lt;em&gt;t&lt;/em&gt; distributions so that it generates fewer extreme innovations:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# nbr of obs
tdf &amp;lt;- 10

mcdf10 &amp;lt;- mcbeta(n, mc, k, tdf)
mcdf10 &amp;lt;- melt(mcdf10)

colnames(mcdf10) &amp;lt;- c(&amp;quot;iteration&amp;quot;, &amp;quot;a&amp;quot;, &amp;quot;value&amp;quot;, &amp;quot;residuals&amp;quot;)

gg.mcdf10 &amp;lt;- ggplot(mcdf10, aes(x = value, fill = residuals)) + geom_histogram(binwidth = 0.005, 
    alpha = 0.5, position = &amp;quot;identity&amp;quot;)
print(gg.mcdf10)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2014-02-17-IE14-W23_rmd/mcdf10.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With 10 degrees of freedom, the two distributions are almost overlapping, the normal approximation is a good one in this case.&lt;/p&gt;

&lt;h2 id=&quot;question&quot;&gt;Question:&lt;/h2&gt;
&lt;p&gt;Can you setup a similar experiment to evaluate the size and the power of the &lt;em&gt;t&lt;/em&gt; test?&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/ie/IE14-W23&quot;&gt;Week 2-3: R squared, Testing, and Monte Carlo.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on February 17, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Introductory Econometrics (inleiding econometrie 1) 2014]]></title>
  <link>/teaching/IE14.html</link>
  <id>/teaching/IE2014_main</id>
  <updated>2014-01-27T00:00:00-00:00</updated>
  <published>2014-01-27T00:00:00+01:00</published>
  
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;I will publish every week a post summarizing the material covered in the week with some extra questions and a focus on computing.&lt;/p&gt;

&lt;p&gt;All the code in these posts is &lt;em&gt;R&lt;/em&gt; code. &lt;em&gt;R&lt;/em&gt; allows me to easily share code with you, which you can run and modify. The simplest way to do that is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install &lt;a href=&quot;http://www.rstudio.com&quot;&gt;Rstudio&lt;/a&gt; (free, open source, compatible with all platform),&lt;/li&gt;
  &lt;li&gt;Copy/paste the pieces of code below in Rstudio’s console,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;that’s it.&lt;/p&gt;

&lt;p&gt;These post also contains some computational questions. I encourage you to give these questions a try: if you really understand something, then you should be able to program it. Use whichever language/package you feel most comfortable with.&lt;/p&gt;

&lt;p&gt;I have enabled comments. I encourage you to use those to ask questions and reply to open questions if you know the answer. The comments are anonymous and not necessarily linked to your &lt;em&gt;vu&lt;/em&gt; email, please keep it clean.&lt;/p&gt;

&lt;article&gt;
&lt;h3&gt;&lt;a href=&quot;/ie/IE14-W6&quot;&gt;Week 6: Empirical size and power of Likelihood based tests.&lt;/a&gt;&lt;/h3&gt;
&lt;!---&lt;span class=&quot;entry-date&quot;&gt;&lt;time datetime=&quot;2014-03-09T00:00:00+01:00&quot;&gt;March 2014&lt;/time&gt;&lt;/span&gt;--&gt;
&lt;/article&gt;

&lt;article&gt;
&lt;h3&gt;&lt;a href=&quot;/ie/IE14-W23&quot;&gt;Week 2-3: R squared, Testing, and Monte Carlo.&lt;/a&gt;&lt;/h3&gt;
&lt;!---&lt;span class=&quot;entry-date&quot;&gt;&lt;time datetime=&quot;2014-02-17T00:00:00+01:00&quot;&gt;February 2014&lt;/time&gt;&lt;/span&gt;--&gt;
&lt;/article&gt;

&lt;article&gt;
&lt;h3&gt;&lt;a href=&quot;/ie/IE14-W1&quot;&gt;Week 1: Multiple Regressions and Matrix Notation.&lt;/a&gt;&lt;/h3&gt;
&lt;!---&lt;span class=&quot;entry-date&quot;&gt;&lt;time datetime=&quot;2014-01-27T00:00:00+01:00&quot;&gt;January 2014&lt;/time&gt;&lt;/span&gt;--&gt;
&lt;/article&gt;


  &lt;p&gt;&lt;a href=&quot;/teaching/IE14.html&quot;&gt;Introductory Econometrics (inleiding econometrie 1) 2014&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on January 27, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Week 1: Multiple Regressions and Matrix Notation.]]></title>
  <link>/ie/IE14-W1</link>
  <id>/ie/IE14-W1</id>
  <published>2014-01-27T00:00:00+01:00</published>
  <updated>2014-01-27T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;h1 id=&quot;regressions-with-multiple-variables-and-matrix-notation&quot;&gt;Regressions with multiple variables and matrix notation&lt;/h1&gt;

&lt;p&gt;This week we discussed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Regressions with multiple variables.&lt;/li&gt;
  &lt;li&gt;The OLS in natrix notation.&lt;/li&gt;
  &lt;li&gt;Properties of the OLS  estimator.&lt;/li&gt;
  &lt;li&gt;Omitted variable bias.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post will focus on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generating artificial data.&lt;/li&gt;
  &lt;li&gt;Use matrices in the code.&lt;/li&gt;
  &lt;li&gt;Estimate the OLS using built in functions&lt;/li&gt;
  &lt;li&gt;Manually estimate the OLS.&lt;/li&gt;
  &lt;li&gt;Calculate the parameters standard errors.&lt;/li&gt;
  &lt;li&gt;Plot the density of the residuals.&lt;/li&gt;
  &lt;li&gt;Load the wage data and compute regressions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can get help on any function by typing &lt;em&gt;?FunctionName&lt;/em&gt; in the &lt;em&gt;R&lt;/em&gt; console, try &lt;em&gt;?dim&lt;/em&gt; for example.&lt;/p&gt;

&lt;h3 id=&quot;generating-data-following-a-regression-model-with-multiple-variables&quot;&gt;Generating data following a regression model with multiple variables&lt;/h3&gt;
&lt;p&gt;Let us assume the response variable &lt;em&gt;y&lt;/em&gt; follows a regression model with 3 explanatory variables:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
	y &amp; = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon \\
	y &amp; = (\iota,x_1,x_2,x_3)\beta + \epsilon \\
	y &amp; = X\beta + \epsilon
	\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where \( \epsilon_i \sim \mathcal{N}(0,1)\), \( \iota \) is a vector of ones (&lt;em&gt;i.e.&lt;/em&gt; the constant), and \( \beta = (1,0.5,1,-1)’\) is the \(4\times 1\) vector of parameters.&lt;/p&gt;

&lt;p&gt;We want to generate data following this regression model with &lt;em&gt;k=3&lt;/em&gt; regressors (plus the constant) and &lt;em&gt;n=100&lt;/em&gt; observations. We generate a matrix &lt;em&gt;X&lt;/em&gt; of dimensions \(n\times k\) filled with random normal values. For that we need the following &lt;em&gt;R&lt;/em&gt; functions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;matrix()&lt;/em&gt; to generate a matrix.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;rnrom()&lt;/em&gt; to generate \(\mathcal{N}(0,1)\) random numbers.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;dim()&lt;/em&gt; to print the dimensions of a matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Choosing the seed
set.seed(666)

# Setting up the number of variables and observations:
k &amp;lt;- 3
n &amp;lt;- 100

# Creating the matrix of regressors:
X &amp;lt;- matrix(rnorm(k * n), ncol = k, nrow = n)

# Creating the vector of parameters beta:
beta &amp;lt;- matrix(c(0.5, 1, -1), ncol = 1, nrow = 3)

# printing the dimensions of X:
dim(X)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## [1] 100   3&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can now generate the variable &lt;em&gt;y&lt;/em&gt; using the matrix multiplication operator &lt;em&gt;%*%&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# First I generate the innovations:
eps &amp;lt;- rnorm(n)

# Then y:
y &amp;lt;- 1 + X %*% beta + eps

# And check that the dimensions make sense
dim(y)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## [1] 100   1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;estimating-beta-by-ols&quot;&gt;Estimating \(\beta\) by OLS&lt;/h3&gt;

&lt;p&gt;The generic function to estimate OLS in &lt;em&gt;R&lt;/em&gt; is called &lt;em&gt;lm()&lt;/em&gt;, notice that it automatically adds a constant to our model.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# estimate the model.
ols.lm &amp;lt;- lm(y ~ X)
# print the results.
summary(ols.lm)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.905 -0.626 -0.110  0.675  2.171 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   0.9873     0.0987   10.00  &amp;lt; 2e-16 ***
## X1            0.4840     0.0972    4.98  2.8e-06 ***
## X2            1.2524     0.0881   14.21  &amp;lt; 2e-16 ***
## X3           -0.9078     0.0986   -9.20  7.6e-15 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.979 on 96 degrees of freedom
## Multiple R-squared:  0.78,	Adjusted R-squared:  0.773 
## F-statistic:  113 on 3 and 96 DF,  p-value: &amp;lt;2e-16&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The summary of the &lt;em&gt;lm()&lt;/em&gt; function shows many statistics that we will learn to understand throughout the course. If you try &lt;em&gt;plot(ols.lm)&lt;/em&gt; you will also get a number of useful plots, but… This is all too easy!&lt;/p&gt;

&lt;p&gt;It is more fun to code the OLS estimator ourselves. Remember the formula for the OLS estimator \(\hat{\beta}\) of \(\beta\) in matrix form: \(\hat{\beta}=\left(X’X\right)^{-1}X’y\). For that we need only two functions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The transpose function: &lt;em&gt;t()&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;The matrix inversion function &lt;em&gt;solve()&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also need to remember adding a constant to X using the &lt;em&gt;cbind()&lt;/em&gt; function.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Adding the constant
X &amp;lt;- cbind(1, X)

# Estimating beta hat
beta.hat &amp;lt;- solve(t(X) %*% X) %*% t(X) %*% y
print(beta.hat)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##         [,1]
## [1,]  0.9873
## [2,]  0.4840
## [3,]  1.2524
## [4,] -0.9078&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The estimates of \(\hat{\beta}\) are the same using both methods, that is nice.&lt;/p&gt;

&lt;h2 id=&quot;question&quot;&gt;Question:&lt;/h2&gt;
&lt;p&gt;Compute the standard errors of the residuals and of the OLS estimator. To do this, you will need to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Compute the OLS residuals:
    &lt;ul&gt;
      &lt;li&gt;compute the fit \( \hat{y}=X\hat{\beta} \),&lt;/li&gt;
      &lt;li&gt;compute the residuals \( \hat{\epsilon} = y - \hat{y} \). This should be a vector of the same length as &lt;em&gt;y&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Compute \(  \hat{\sigma}^2=\frac{\hat{\epsilon}’\hat{\epsilon}}{n-(k+1)}  \), to get the standard error you need to use the &lt;em&gt;sqrt()&lt;/em&gt; function on \(\hat{\sigma}^2\)&lt;/li&gt;
  &lt;li&gt;Get the individual standard errors:
    &lt;ul&gt;
      &lt;li&gt;Get the diagonal of \((X’X)^{-1}\) using &lt;em&gt;diag()&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Scale its square-root by \(\hat{\sigma}\). You should get a vector of standard errors of the same length as &lt;em&gt;beta.hat&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;To compare the output on your machine with the one in this post, make sure you have set the same seed as I have.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Computing the residuals
y.hat &amp;lt;- X %*% beta.hat
e.hat &amp;lt;- y - y.hat

# Residual standard error
sig2 &amp;lt;- (t(e.hat) %*% e.hat)/(n - (k + 1))
res.se &amp;lt;- sqrt(sig2)
cat(&amp;quot;\n residual standard errors:&amp;quot;, res.se, sep = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
##  residual standard errors:0.9785&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Parameter std. err.
diag.xx &amp;lt;- diag(solve(t(X) %*% X))
beta.se &amp;lt;- res.se * sqrt(diag.xx)
cat(&amp;quot;\n Parameters standard errors:&amp;quot;, beta.se, sep = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
##  Parameters standard errors:0.098690.097170.088130.09863&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;visualizing-the-results&quot;&gt;Visualizing the results.&lt;/h3&gt;

&lt;p&gt;Let evaluate the quality of our model graphically. To do that we can look at a scatter plot of the observed values and the residuals.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Creating a data frame with the resituals, and the observed and fitted
# values.
ols.man &amp;lt;- data.frame(residuals = e.hat, fitted = y.hat, observed = y)
# Loading a plotting library.
require(ggplot2)
sqplot &amp;lt;- ggplot(ols.man, aes(y = observed, x = residuals)) + geom_point()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;print(sqplot)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2014-01-27-IE14-W1_rmd/fig-sq.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we can also look at the density of our residuals.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;densplot &amp;lt;- ggplot(ols.man, aes(x = residuals)) + geom_density()
print(densplot)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/figs/2014-01-27-IE14-W1_rmd/fig-dens.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;question-1&quot;&gt;Question:&lt;/h2&gt;

&lt;p&gt;Does this look like a Gaussian density to you? What happens when you re-run the code increasing the sample size &lt;em&gt;n&lt;/em&gt; to 1000?&lt;/p&gt;

&lt;h3 id=&quot;multiple-regression-with-the-wage-data&quot;&gt;Multiple regression with the wage data.&lt;/h3&gt;

&lt;p&gt;First we load the data and check that it looks as expected.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Loading the data.
wage &amp;lt;- read.csv(&amp;quot;../data/wages.csv&amp;quot;)
# Printing the first rows of wage.
head(wage)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##   X EXPER MALE SCHOOL  WAGE
## 1 1     9    0     13 6.315
## 2 2    12    0     12 5.480
## 3 3    11    0     11 3.642
## 4 4     9    0     14 4.593
## 5 5     8    0     14 2.418
## 6 6     9    0     14 2.094&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we can regress &lt;em&gt;WAGE&lt;/em&gt; on &lt;em&gt;SCHOOL&lt;/em&gt; and &lt;em&gt;EXPER&lt;/em&gt;:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# regression of wage on school and exper.
mod1 &amp;lt;- lm(WAGE ~ SCHOOL + EXPER, data = wage)
# printing the results of the regression.
summary(mod1)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
## Call:
## lm(formula = WAGE ~ SCHOOL + EXPER, data = wage)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -6.88  -1.99  -0.52   1.39  34.91 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  -2.4767     0.4700   -5.27  1.5e-07 ***
## SCHOOL        0.5992     0.0334   17.94  &amp;lt; 2e-16 ***
## EXPER         0.1573     0.0242    6.51  8.9e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.12 on 3291 degrees of freedom
## Multiple R-squared:  0.0915,	Adjusted R-squared:  0.0909 
## F-statistic:  166 on 2 and 3291 DF,  p-value: &amp;lt;2e-16&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;question-2&quot;&gt;Question:&lt;/h2&gt;
&lt;p&gt;Try estimating other models using different combination of variables.&lt;/p&gt;

&lt;h3 id=&quot;transformations-of-the-data&quot;&gt;Transformations of the data&lt;/h3&gt;

&lt;p&gt;Maybe the number of years at school does not enter linearily in the model. We can expand the model with a quadratic term:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;# Create a variable with the square of the number of school years.
wage$SCHOOL2 &amp;lt;- wage$SCHOOL^2
# Estimating the model and printing the results.
mod2 &amp;lt;- lm(WAGE ~ SCHOOL + SCHOOL2 + EXPER, data = wage)
summary(mod2)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
## Call:
## lm(formula = WAGE ~ SCHOOL + SCHOOL2 + EXPER, data = wage)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -7.33  -1.96  -0.51   1.38  34.97 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   3.1149     1.5475    2.01  0.04421 *  
## SCHOOL       -0.3840     0.2614   -1.47  0.14201    
## SCHOOL2       0.0433     0.0114    3.79  0.00015 ***
## EXPER         0.1409     0.0245    5.75  9.8e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.11 on 3290 degrees of freedom
## Multiple R-squared:  0.0954,	Adjusted R-squared:  0.0946 
## F-statistic:  116 on 3 and 3290 DF,  p-value: &amp;lt;2e-16&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;question-3&quot;&gt;Question:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Interpret the results above.&lt;/li&gt;
  &lt;li&gt;Maybe the number of years of schooling has a different effect for males and for females. How would you investigate this?&lt;/li&gt;
&lt;/ol&gt;

  &lt;p&gt;&lt;a href=&quot;/ie/IE14-W1&quot;&gt;Week 1: Multiple Regressions and Matrix Notation.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on January 27, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Oracle inequalities for high dimensional vector autoregressions.]]></title>
  <link>/papers/oracle-var</link>
  <id>/papers/oracle-var</id>
  <published>2013-11-22T00:00:00+01:00</published>
  <updated>2013-11-22T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;&lt;strong&gt;Accepted, &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0304407615000378&quot;&gt;Journal of Econometrics&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Download the paper &lt;a href=&quot;/papers/oracle_var.pdf&quot;&gt;pdf&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract:&lt;/h2&gt;

&lt;p&gt;This paper establishes non-asymptotic oracle inequalities for the prediction error and estimation accuracy of the LASSO in stationary vector autoregressive models. These inequalities are used to establish consistency of the LASSO even when the number of parameters is of a much larger order of magnitude than the sample size. We also give conditions under which no relevant variables are excluded. Next, non-asymptotic probabilities are given for the Adaptive LASSO to select the correct sparsity pattern. We then give conditions under which the Adaptive LASSO reveals the correct sparsity pattern asymptotically. We establish that the estimates of the non-zero coefficients are asymptotically equivalent to the oracle assisted least squares estimator. This is used to show that the rate of convergence of the estimates of the non-zero coefficients is identical to the one of least squares only including the relevant covariates.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/papers/oracle-var&quot;&gt;Oracle inequalities for high dimensional vector autoregressions.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on November 22, 2013.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Oracle Efficient estimation and Forecasting with the Adaptive Lasso and the Adaptive Group Lasso in Vector Autoregressions.]]></title>
  <link>/papers/oracle-forecasting</link>
  <id>/papers/oracle-forecasting</id>
  <published>2013-11-22T00:00:00+01:00</published>
  <updated>2013-11-22T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;&lt;strong&gt;Published as: Callot, Laurent A. F., and Anders Bredahl Kock, 2014, Oracle Efficient Estimation and Forecasting with the Adaptive Lasso and the Adaptive Group Lasso in Vector Autoregressions, in N. Haldrup, M. Meitz, and P. Saikkonen (eds.), Essays in Nonlinear Time Series Econometrics, Oxford University Press.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Download the paper &lt;a href=&quot;/papers/oracle_forecast.pdf&quot;&gt;pdf&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract:&lt;/h2&gt;
&lt;p&gt;We show that the adaptive Lasso (aLasso) and the adaptive group Lasso (agLasso) are oracle efficient in stationary vector autoregressions where the number of parameters per equation is smaller than the number of observations. In particular, this means that the parameters are estimated consistently at a √T -rate, that the truly zero parameters are classified as such asymptotically and that the non-zero parameters are estimated as efficiently as if only the relevant variables had been included in the model from the outset. The group adaptive Lasso differs from the adaptive Lasso by dividing the covariates into groups whose members are all relevant or all irrelevant. Both estimators have the property that they perform variable selection and estimation in one step. We evaluate the forecasting accuracy of these estimators for a large set of macroeconomic variables. The plain Lasso is found to be the most precise procedure overall. The adaptive and the adaptive group Lasso are less stable but mostly perform at par with common factor models.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/papers/oracle-forecasting&quot;&gt;Oracle Efficient estimation and Forecasting with the Adaptive Lasso and the Adaptive Group Lasso in Vector Autoregressions.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on November 22, 2013.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Estimating and Testing for a Common Co-Integration Space in Large Panels of Co-Integrated VARs.]]></title>
  <link>/papers/CCS</link>
  <id>/papers/CCS</id>
  <published>2013-11-22T00:00:00+01:00</published>
  <updated>2013-11-22T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;Download the paper &lt;a href=&quot;/papers/pcvar_ccs.pdf&quot;&gt;pdf&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract:&lt;/h2&gt;
&lt;p&gt;This paper proposes a maximum likelihood estimator for a common co-integration space in large panels of co-integrated Vector Autoregressive models. The method pioneered by Pesaran (2006) and further refined in Dees, Mauro, Pesaran, and Smith (2007) is used to reduce the dimension of the parameter space of the model and control for cross section dependence. The common co-integration space is estimated using standard optimization methods.. Test statistics for the existence of a common co-integration space against the hypothesis of heterogeneous co-integration spaces are derived. A bootstrap algorithm to generat pseudo data under the hypothesis of a common co-integration space is proposed, and bootstrap test statistics are derived. Identification of the co-integration vectors of the panel is also discussed.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;/papers/CCS&quot;&gt;Estimating and Testing for a Common Co-Integration Space in Large Panels of Co-Integrated VARs.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on November 22, 2013.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[A Bootstrap Co-Integration Rank Test for Panels of Co-Integrated VARs.]]></title>
  <link>/papers/a-bootstrap-panel-co-integration-rank-test</link>
  <id>/papers/a-bootstrap-panel-co-integration-rank-test</id>
  <published>2013-11-21T00:00:00+01:00</published>
  <updated>2013-11-21T00:00:00+01:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;Download the paper: &lt;a href=&quot;/papers/pcvar_rank.pdf&quot;&gt;pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Download the replication material: &lt;a href=&quot;/papers/pcvar-replication.zip&quot;&gt;zip (9 MB)&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract:&lt;/h2&gt;

&lt;p&gt;This paper proposes a sequential procedure to determine the co-integration rank of panels of co-integrated VAR models. The rank is defined as the number of co-integration vectors within an individual system and between that system and the rest of the panel. The method proposed by Pesaran (2006) is used to control for cross section dependence and reduce the dimension of the parameter space. A bootstrap procedure derived from the bootstrap rank test of Cavaliere, Rahbek, and Taylor (2012) is used to compute the empirical distribution of the trace test statistics and construct a panel trace test statistic based on pooling of the individual p-values. The small sample properties of these tests are documented by means of Monte Carlo. An empirical application illustrates the usefulness of this tests.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/papers/a-bootstrap-panel-co-integration-rank-test&quot;&gt;A Bootstrap Co-Integration Rank Test for Panels of Co-Integrated VARs.&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on November 21, 2013.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Syntax Highlighting Post]]></title>
  <link>/articles/code-highlighting-post</link>
  <id>/articles/code-highlighting-post</id>
  <published>2013-08-16T00:00:00+02:00</published>
  <updated>2013-08-16T00:00:00+02:00</updated>
  <author>
    <name>Laurent Callot</name>
    <uri></uri>
    <email>lcallot@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;Syntax highlighting is a feature that displays source code, in different colors and fonts according to the category of terms. This feature facilitates writing in a structured language such as a programming language or a markup language as both structures and syntax errors are visually distinct. Highlighting does not affect the meaning of the text itself; it is intended only for human readers.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;pygments-code-blocks&quot;&gt;Pygments Code Blocks&lt;/h3&gt;

&lt;p&gt;To modify styling and highlight colors edit &lt;code&gt;/assets/less/pygments.less&lt;/code&gt; and compile &lt;code&gt;main.less&lt;/code&gt; with your favorite preprocessor. Or edit &lt;code&gt;main.css&lt;/code&gt; if that’s your thing, the classes you want to modify all begin with &lt;code&gt;.highlight&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-css&quot; data-lang=&quot;css&quot;&gt;#container {
    float: left;
    margin: 0 -240px 0 0;
    width: 100%;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&amp;lt;nav class=&amp;quot;pagination&amp;quot; role=&amp;quot;navigation&amp;quot;&amp;gt;
    {% if page.previous %}
        &amp;lt;a href=&amp;quot;{{ site.url }}{{ page.previous.url }}&amp;quot; class=&amp;quot;btn&amp;quot; title=&amp;quot;{{ page.previous.title }}&amp;quot;&amp;gt;Previous article&amp;lt;/a&amp;gt;
    {% endif %}
    {% if page.next %}
        &amp;lt;a href=&amp;quot;{{ site.url }}{{ page.next.url }}&amp;quot; class=&amp;quot;btn&amp;quot; title=&amp;quot;{{ page.next.title }}&amp;quot;&amp;gt;Next article&amp;lt;/a&amp;gt;
    {% endif %}
&amp;lt;/nav&amp;gt;&amp;lt;!-- /.pagination --&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;module Jekyll
  class TagIndex &amp;lt; Page
    def initialize(site, base, dir, tag)
      @site = site
      @base = base
      @dir = dir
      @name = 'index.html'
      self.process(@name)
      self.read_yaml(File.join(base, '_layouts'), 'tag_index.html')
      self.data['tag'] = tag
      tag_title_prefix = site.config['tag_title_prefix'] || 'Tagged: '
      tag_title_suffix = site.config['tag_title_suffix'] || '&amp;amp;#8211;'
      self.data['title'] = &amp;quot;#{tag_title_prefix}#{tag}&amp;quot;
      self.data['description'] = &amp;quot;An archive of posts tagged #{tag}.&amp;quot;
    end
  end
end&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;standard-code-block&quot;&gt;Standard Code Block&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;nav class=&quot;pagination&quot; role=&quot;navigation&quot;&amp;gt;
    {% if page.previous %}
        &amp;lt;a href=&quot;{{ site.url }}{{ page.previous.url }}&quot; class=&quot;btn&quot; title=&quot;{{ page.previous.title }}&quot;&amp;gt;Previous article&amp;lt;/a&amp;gt;
    {% endif %}
    {% if page.next %}
        &amp;lt;a href=&quot;{{ site.url }}{{ page.next.url }}&quot; class=&quot;btn&quot; title=&quot;{{ page.next.title }}&quot;&amp;gt;Next article&amp;lt;/a&amp;gt;
    {% endif %}
&amp;lt;/nav&amp;gt;&amp;lt;!-- /.pagination --&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;fenced-code-blocks&quot;&gt;Fenced Code Blocks&lt;/h3&gt;

&lt;p&gt;To modify styling and highlight colors edit &lt;code&gt;/assets/less/coderay.less&lt;/code&gt; and compile &lt;code&gt;main.less&lt;/code&gt; with your favorite preprocessor. Or edit &lt;code&gt;main.css&lt;/code&gt; if that’s your thing, the classes you want to modify all begin with &lt;code&gt;.coderay&lt;/code&gt;. Line numbers and a few other things can be modified in &lt;code&gt;_config.yml&lt;/code&gt; under &lt;code&gt;coderay&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-css&quot;&gt;#container {
    float: left;
    margin: 0 -240px 0 0;
    width: 100%;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;nav class=&quot;pagination&quot; role=&quot;navigation&quot;&amp;gt;
    {% if page.previous %}
        &amp;lt;a href=&quot;{{ site.url }}{{ page.previous.url }}&quot; class=&quot;btn&quot; title=&quot;{{ page.previous.title }}&quot;&amp;gt;Previous article&amp;lt;/a&amp;gt;
    {% endif %}
    {% if page.next %}
        &amp;lt;a href=&quot;{{ site.url }}{{ page.next.url }}&quot; class=&quot;btn&quot; title=&quot;{{ page.next.title }}&quot;&amp;gt;Next article&amp;lt;/a&amp;gt;
    {% endif %}
&amp;lt;/nav&amp;gt;&amp;lt;!-- /.pagination --&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;module Jekyll
  class TagIndex &amp;lt; Page
    def initialize(site, base, dir, tag)
      @site = site
      @base = base
      @dir = dir
      @name = 'index.html'
      self.process(@name)
      self.read_yaml(File.join(base, '_layouts'), 'tag_index.html')
      self.data['tag'] = tag
      tag_title_prefix = site.config['tag_title_prefix'] || 'Tagged: '
      tag_title_suffix = site.config['tag_title_suffix'] || '&amp;amp;#8211;'
      self.data['title'] = &quot;#{tag_title_prefix}#{tag}&quot;
      self.data['description'] = &quot;An archive of posts tagged #{tag}.&quot;
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Syntax_highlighting&quot;&gt;http://en.wikipedia.org/wiki/Syntax_highlighting&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;

  &lt;p&gt;&lt;a href=&quot;/articles/code-highlighting-post&quot;&gt;Syntax Highlighting Post&lt;/a&gt; was originally published by Laurent Callot at &lt;a href=&quot;&quot;&gt;Laurent's Research Page&lt;/a&gt; on August 16, 2013.&lt;/p&gt;</content>
</entry>

</feed>
